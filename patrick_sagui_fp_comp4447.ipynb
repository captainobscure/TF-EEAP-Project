{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Data has been sourced from Kaggle.\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "From Kaggle: You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n",
    "\n",
    "toxic\n",
    "severe_toxic\n",
    "obscene\n",
    "threat\n",
    "insult\n",
    "identity_hate\n",
    "\n",
    "Goal is to use word embeddings and a Recurent Neural Network to predict if a comment is toxic vs non-toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from statistics import mean\n",
    "from itertools import groupby\n",
    "from contractions import contractions_dict\n",
    "from autocorrect import Speller\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training data\n",
    "\n",
    "Data set was obtained from kaggle Toxic Comment Classification Challenge.\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "For the purposes of this study we will only be using the comment_text and the toxic flag.  Since the the other categories are not exclusive they can be hard to categorize, and as such will be removed.\n",
    "\n",
    "See an exmaple of some of the data below.\n",
    "\n",
    "### Warning this is currently unfiltered internet data, as such expect to see some words, comments phrases that are potentially offensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7880</th>\n",
       "      <td>14f8b9fd4928799e</td>\n",
       "      <td>I am not employed by the town, nor have I ever been. Even if they sections I added are inappropriate for the article, please explain how they were added by members the town government when I was the one who did so.\\n\\nThe media and education sections were based off of the Charlottesville, Virginia article. I am going to add the education section back since you said it was okay, as well as adding the mayor and town manager back to the info box.\\n\\nThanks for the criticisms.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63307</th>\n",
       "      <td>a966a5a5c70b0531</td>\n",
       "      <td>Well that is not the case, almost every writer/ comedian is known as English/Welsh or Scottish, except Ricky and Stephen Merchant. Ricky should be stated as a English comedian, in the introduction. In the infobox under nationality it should be British.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81381</th>\n",
       "      <td>d9aeef95410e6267</td>\n",
       "      <td>Colonial Revival garden \\n\\nThanks for fixing it up! The article had been on my watchlist because it had some wikifying concerns and titling issues. It's a much better article now!  (t â¢ c)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65257</th>\n",
       "      <td>ae9b09ec9bf020f5</td>\n",
       "      <td>Talk about inane support for bad moderators. No wonder no collegiate or self respecting person takes Wikipedia seriously, it's worse than an internet forum around here. Whatever dudes, continue to play your game with someone else's lives. I hope your constant reassurance to the Japanese that there was wide-spread anti-semantic behavior directed at them during their 2011 tsunami gets you targeted by a hate group. Seriously guys, burn in hell for being miserable moderators that only aim to spread lies to somehow record popular opinion of a time that was actually not even on anyone's mind during the incident.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16305</th>\n",
       "      <td>2afd0b8c142132e1</td>\n",
       "      <td>CrankScorpion\\n\\nYou imaged these basaltic columns near Lamont-Doherty Earth Observatory in Palisades New York. I was unaware of these columns and I would like to vist them and image them.\\nWhat road are these columns on? I wish to visit. I live in The Bronx. Can you send me directions to these columns?\\n\\nThanks\\n\\n24.44.189.124</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  \\\n",
       "7880   14f8b9fd4928799e   \n",
       "63307  a966a5a5c70b0531   \n",
       "81381  d9aeef95410e6267   \n",
       "65257  ae9b09ec9bf020f5   \n",
       "16305  2afd0b8c142132e1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "7880                                                                                                                                           I am not employed by the town, nor have I ever been. Even if they sections I added are inappropriate for the article, please explain how they were added by members the town government when I was the one who did so.\\n\\nThe media and education sections were based off of the Charlottesville, Virginia article. I am going to add the education section back since you said it was okay, as well as adding the mayor and town manager back to the info box.\\n\\nThanks for the criticisms.   \n",
       "63307                                                                                                                                                                                                                                                                                                                                                                           Well that is not the case, almost every writer/ comedian is known as English/Welsh or Scottish, except Ricky and Stephen Merchant. Ricky should be stated as a English comedian, in the introduction. In the infobox under nationality it should be British.   \n",
       "81381                                                                                                                                                                                                                                                                                                                                                                                                                                        Colonial Revival garden \\n\\nThanks for fixing it up! The article had been on my watchlist because it had some wikifying concerns and titling issues. It's a much better article now!  (t â¢ c)   \n",
       "65257  Talk about inane support for bad moderators. No wonder no collegiate or self respecting person takes Wikipedia seriously, it's worse than an internet forum around here. Whatever dudes, continue to play your game with someone else's lives. I hope your constant reassurance to the Japanese that there was wide-spread anti-semantic behavior directed at them during their 2011 tsunami gets you targeted by a hate group. Seriously guys, burn in hell for being miserable moderators that only aim to spread lies to somehow record popular opinion of a time that was actually not even on anyone's mind during the incident.   \n",
       "16305                                                                                                                                                                                                                                                                                            CrankScorpion\\n\\nYou imaged these basaltic columns near Lamont-Doherty Earth Observatory in Palisades New York. I was unaware of these columns and I would like to vist them and image them.\\nWhat road are these columns on? I wish to visit. I live in The Bronx. Can you send me directions to these columns?\\n\\nThanks\\n\\n24.44.189.124   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "7880       0             0        0       0       0              0  \n",
       "63307      0             0        0       0       0              0  \n",
       "81381      0             0        0       0       0              0  \n",
       "65257      1             0        0       0       0              0  \n",
       "16305      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting to display full text in output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "random.seed(30)\n",
    "\n",
    "\n",
    "#read the dataset\n",
    "train_raw_df = pd.read_csv(\"train.csv\",encoding = \"ISO-8859-1\")\n",
    "#sample 5 random rows\n",
    "train_raw_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the comments have missspellings, typos, weird characters, bad use of capitals, numbers, urls and all the other things that might show up in free text on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Basics\n",
    "In an effort to keep this problem managable from a memory perspective, we are going to filter some of the longer comments out of the code, this will help the model later with memory, but also makes the problem a little easier on home computers.  This is obviously a practical decision rather than a statistical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments: 159571\n",
      "Mean of comment length: 67.30\n",
      "Std Deviation of comment length: 99.25\n",
      "Number of comments filtered: 0\n",
      "\n",
      "toxic\n",
      "0    144277\n",
      "1     15294\n",
      "Name: id, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='id'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_raw_data_len = len(train_raw_df)\n",
    "print(f\"Number of comments: {train_raw_data_len}\")\n",
    "print(f\"Mean of comment length: {train_raw_df.comment_text.str.split().str.len().mean():.2f}\")\n",
    "print(f\"Std Deviation of comment length: {train_raw_df.comment_text.str.split().str.len().std():.2f}\")\n",
    "\n",
    "# remove any comments longer than 3 std deviations above the mean\n",
    "# cuttoff = train_raw_df.comment_text.str.split().str.len().std()*2.2 + train_raw_df.comment_text.str.split().str.len().mean()\n",
    "# train_raw_df = train_raw_df[train_raw_df.comment_text.str.split().str.len().lt(cuttoff)]\n",
    "\n",
    "print(f\"Number of comments filtered: {train_raw_data_len - len(train_raw_df)}\\n\")\n",
    "\n",
    "print(train_raw_df.groupby(\"toxic\").id.count())\n",
    "\n",
    "%matplotlib inline\n",
    "sns.barplot([\"Not-Toxic\",\"Toxic\"],train_raw_df.groupby(\"toxic\").id.count(), palette=sns.cubehelix_palette(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unused Colmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = train_raw_df.drop(['id', 'severe_toxic','obscene','threat','insult','identity_hate'], axis=1)\n",
    "clean_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't be used again, and we don't want excess memory being used needlessly.\n",
    "del train_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing\n",
    "\n",
    "Here we are going to attempt to clean up some of the data, removing bad characters, potential misspellings, links etc.  Since we are using GLoVe for this it is possible to also learn from emojis, characters, stopwords and symbols so we need to be careful about overcleaning because in the processes we are potentially loosing information.  For example :) might be an indication of a joke, so removing the symbol potentially context.  Since GLoVe learns its embeddings its possible to associate the \"word\" :) with a vector space, hense we don't want to remove it.\n",
    "\n",
    "That said, for purposes of this project, we will split contractions, remove links, turn things to lower, and remove newlines at a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is not cool, I am\n"
     ]
    }
   ],
   "source": [
    "#simple test for expanding contractions\n",
    "expanded_contraction = expand_contractions(\"this isn't cool, I'm\",contractions_dict)\n",
    "print(expanded_contraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Display latex example of stop words for report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'me', 'my', 'myself', 'we'], ['ours', 'ourselves', 'you', \"you're\", \"you've\"], [\"you'd\", 'your', 'yours', 'yourself', 'yourselves'], ['he', 'him', 'his', 'himself', 'she']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\\\begin{tabular}{lllll}\\n\\\\hline\\n i     & me        & my    & myself   & we         \\\\\\\\\\n ours  & ourselves & you   & you're   & you've     \\\\\\\\\\n you'd & your      & yours & yourself & yourselves \\\\\\\\\\n he    & him       & his   & himself  & she        \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "print([stop_words[:5],stop_words[6:11],stop_words[12:17],stop_words[17:22]])\n",
    "\n",
    "tabulate.tabulate([stop_words[:5],stop_words[6:11],stop_words[12:17],stop_words[17:22]], tablefmt='latex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Checking\n",
    "Attempt to fix obvious typos, so \"hte\" to \"the\" utilizing the autocorrect package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'the', 'silly', 'thing', 'here', 'asap', ',', 'u', 'monkey']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correct_spelling(txt_tokens):\n",
    "    check = Speller(lang='en')\n",
    "    correct_words = [check(w) for w in  txt_tokens]\n",
    "    return correct_words\n",
    "\n",
    "##simple check\n",
    "text = (nltk.word_tokenize(\"get hte sillly hting here asap, u monkiy\"))\n",
    "text = correct_spelling(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method does a good job at not correcting asap, fixing the obvious miss spellings but not chaning the u (that will be fixed in a later process.  Unfortunately this method is extreemly slow, to the order of days to calculate for the entire set, making this completely untractable.  Also occasionally it will fix words that are not miss spelled but rather are acronyms or names of companies etc.  This could potentially change the meaning of a sentance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'the', 'stupid', 'thing', 'here', 'asap', 'a', 'a', 'monkey']\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(txt_tokens):\n",
    "    for i in range(len(txt_tokens)):\n",
    "        suggestions = sym_spell.lookup(txt_tokens[i], Verbosity.CLOSEST, max_edit_distance=1)\n",
    "        txt_tokens[i] = suggestions[0].term\n",
    "    return txt_tokens\n",
    "\n",
    "text = (nltk.word_tokenize(\"get hte stopid hting here asap, u monkiy\"))\n",
    "text = correct_spelling(text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method for spelling is supposed to be much faster, but still pretty slow.  This method unlike the last one is much more aggressive on making changes to the words.  You can see it changed u to a.  We are going to handle common short hand but since this aggressive it is more likely to change the meaning of a sentance by accident if it changes a persons name, company name, acronymns.  For that purpose spelling will likely be excluded as a tactical choice.  But you can see some functions for managing miss spellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate words\n",
    "Word duplicates showed up a lot in my outliers on the first pass of this method, and as such needed to be filtered.  In a number of occasions you comments with strings of just repeated copy and pasted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lets', 'get', 'this', 'year', 'over', 'with', ',', '2020', 'stinks']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_index = []\n",
    "def drop_duplicate_words(txt_token):\n",
    "    return [x for x, _ in groupby(txt_token)]\n",
    "\n",
    "text = (nltk.word_tokenize(\"lets get get this year over over with, 2020 stinks\"))\n",
    "text = drop_duplicate_words(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Replacement\n",
    "Here we will be removing or replacing text that adds either adds little value, is unlikely to have duplicates or otherwise doesn't make sense.\n",
    "    - url links will be encoded to _url_\n",
    "    - @usernames will be changed to _AT_\n",
    "    - Any text that is not a A-Za-z0-9(),?`'\"_ or newline will be replaced with a space\n",
    "    - All comments will be converted to lower case\n",
    "    - Since specific numbers unlikely hold specific value, all numbers will be changed to # rather than excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># is the number of choice!!! _at_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_url_ hahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>something \"like\"  this</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   A\n",
       "0  # is the number of choice!!! _at_\n",
       "1                        _url_ hahah\n",
       "2            something \"like\"  this "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_df_text_field(df, field_name):\n",
    "    df[field_name] = df[field_name].str.replace(r\"http\\S+\", \"_url_\")\n",
    "    df[field_name] = df[field_name].str.replace(r\"http\", \"_url_\")\n",
    "    df[field_name] = df[field_name].str.replace(r\"@\\S+\", \"_AT_\")\n",
    "    df[field_name] = df[field_name].str.replace(r\"[^A-Za-z0-9(),!?\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[field_name] = df[field_name].str.replace(r\"\\W*\\b\\w{15,}\\b\",\"\")\n",
    "    df[field_name] = df[field_name].str.replace(r\"[0-9]+\",\"#\")\n",
    "    df[field_name] = df[field_name].str.lower()\n",
    "    return df\n",
    "\n",
    "test_df = pd.DataFrame({\"A\": [\"666 is the number of choice!!! @Jerk\", \"http://google.com oneoneoneoneoneoneoneoeoeoeoe hahah\", \"♠>↑â something \\\"like\\\"  this \"]})\n",
    "clean_df_text_field(test_df, \"A\")\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviations\n",
    "\n",
    "Here we will be handling common abbreviations or shorthand \"asap\" to \"as soon as possible\" or \"u\" to \"you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lets', 'get', 'this', 'you', 'as soon as possible', 'laughing out loud']\n"
     ]
    }
   ],
   "source": [
    "slang_dict = dict()\n",
    "with open(\"slang.txt\", \"r\") as slangFile:\n",
    "    fileData = csv.reader(slangFile, delimiter=\"=\")\n",
    "    for row in fileData:\n",
    "        slang_dict[row[0].lower()] = row[1].lower()\n",
    "\n",
    "def correct_abrev(txt_tokens, slang):\n",
    "    def check_abrev(w,slang):\n",
    "        if w in slang:\n",
    "            return slang[w]\n",
    "        else:\n",
    "            return w\n",
    "            \n",
    "    corrected =  [check_abrev(w,slang) for w in  txt_tokens]\n",
    "    return corrected\n",
    "\n",
    "text = correct_abrev(['lets', 'get', 'this', 'u', 'asap', 'lol'],slang_dict)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = expand_contractions(txt,contractions_dict)\n",
    "    txt_tokens = (nltk.word_tokenize(txt))\n",
    "    txt_tokens = drop_duplicate_words(txt_tokens)\n",
    "    txt_tokens = correct_abrev(txt_tokens,slang_dict)\n",
    "    #commented for time constraints also put last in the list so that it doesn't correct abreviations.\n",
    "#     txt_tokens = correct_spelling(txt_tokens)\n",
    "    return \" \".join(txt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "clean_df_text_field(clean_df, \"comment_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n 1\n",
    "bad_labels = []\n",
    "for i in range(len(clean_df.comment_text)):\n",
    "    if i % 5000 == 0:\n",
    "        print('.',end='')\n",
    "    try:\n",
    "        result = clean_text(clean_df.loc[i,\"comment_text\"])\n",
    "        clean_df.loc[i,\"comment_text\"] = result\n",
    "    except:\n",
    "        print(f\"Something went wrong processing: {i}\")\n",
    "        bad_labels.append(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file so you can pick up again here later and skip the long cleaning process\n",
    "# clean_df.to_pickle(\"clean_df_filtered.pkl\")\n",
    "clean_df = pd.read_pickle(\"clean_df_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and New Line Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141367"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = Tokenizer(filters='')\n",
    "token.fit_on_texts(clean_df.comment_text.tolist())\n",
    "vocab_size = len(token.word_index)+1\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "\n",
    "for txt in clean_df.comment_text.tolist():\n",
    "    tx = token.texts_to_sequences([txt])\n",
    "    tx = [item for sublist in tx for item in sublist]\n",
    "    count = Counter(tx) + count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary for reverse word mapping, can be used to reconstruct sentances from sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_token = {value:key for key, value in token.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index   count\n",
      "0    the  371988\n",
      "1      ,  352027\n",
      "2     to  230251\n",
      "3    you  177458\n",
      "4    and  166012\n",
      "5      a  163973\n",
      "6     is  163424\n",
      "7     of  162439\n",
      "8      i  160170\n",
      "9     ``  141947\n",
      "10   not  122447\n",
      "11  that  121944\n",
      "12    it  118576\n",
      "13    ''  117081\n",
      "14    in  106413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'the'),\n",
       " Text(1, 0, ','),\n",
       " Text(2, 0, 'to'),\n",
       " Text(3, 0, 'you'),\n",
       " Text(4, 0, 'and'),\n",
       " Text(5, 0, 'a'),\n",
       " Text(6, 0, 'is'),\n",
       " Text(7, 0, 'of'),\n",
       " Text(8, 0, 'i'),\n",
       " Text(9, 0, '``'),\n",
       " Text(10, 0, 'not'),\n",
       " Text(11, 0, 'that'),\n",
       " Text(12, 0, 'it'),\n",
       " Text(13, 0, \"''\"),\n",
       " Text(14, 0, 'in'),\n",
       " Text(15, 0, '#'),\n",
       " Text(16, 0, 'for'),\n",
       " Text(17, 0, 'this'),\n",
       " Text(18, 0, 'on'),\n",
       " Text(19, 0, ')'),\n",
       " Text(20, 0, 'be'),\n",
       " Text(21, 0, 'are'),\n",
       " Text(22, 0, '('),\n",
       " Text(23, 0, 'have'),\n",
       " Text(24, 0, 'as')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_df = pd.DataFrame.from_dict({rev_token[k]: v for k, v in sorted(count.items(), key=lambda item: item[1],reverse=True)}, orient='index', columns=[\"count\"] ).reset_index()\n",
    "print(count_df[:15])\n",
    "# print(count_df[-10:])\n",
    "# print(count_df['count'].value_counts())\n",
    "\n",
    "%matplotlib inline\n",
    "fig, ax = plt.pyplot.subplots(figsize=(10,5))\n",
    "chart = sns.barplot(x=\"index\", y=\"count\", data=count_df[:25],palette=sns.cubehelix_palette(8))\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step the text will be encoded using the ntlk tokenizer to convert into a list of numbers numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text = token.texts_to_sequences(clean_df.comment_text.tolist())\n",
    "encode_text_y = clean_df.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55, 1168, 12, 18, 11896, 18961, 5, 4914, 68, 696, 8, 1097, 736], [1132, 13, 49]]\n",
      "[['just', 'checking', 'that', 'this', 'sentance', 'splits', 'and', 'returns', 'some', 'type', 'of', 'reasonable', 'result'], ['hopefully', 'it', 'will']]\n"
     ]
    }
   ],
   "source": [
    "encode_text_test = token.texts_to_sequences([\"just checking that this sentance splits properly, and returns some type of reasonable result\", \"hopefully it will\"])\n",
    "print(encode_text_test)\n",
    "decoded_text_test = []\n",
    "for lst in encode_text_test:\n",
    "    decoded_text = []\n",
    "    for code in lst:\n",
    "        decoded_text.append(rev_token[code])\n",
    "    decoded_text_test.append(decoded_text)\n",
    "print(decoded_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment Legnth\n",
    "Here we need to make some practical considerations about comment sizes.  The model used for this project takes the input comments as an array of vectors of equal length.  Knowing this if we take the max size we will be filling most of the array with zeros.  Since comments of extreemly long length are uncommon we should likely take some practical memory considerations and truncate some of the larger sequences at some point.  Below is a graph containing the distribution of text sequence lengths after tokenization.  You can see that there are very few comments larger than 225 words, as such we will stop any comment at 250 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Number of Tokens: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154362"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_tokens = max([len(r) for r in encode_text])\n",
    "# max_comment_len_list = [len(r) for r in clean_df.comment_text.tolist()]\n",
    "# print(max_comment_len_list.index(max(max_comment_len_list)))\n",
    "# print(clean_df.loc[139871,\"comment_text\"])\n",
    "print(\"Max Number of Tokens: \".format(max_tokens))\n",
    "%matplotlib inline\n",
    "sns.set_palette(sns.cubehelix_palette(8))\n",
    "sns.distplot([len(r) for r in encode_text], bins=100, kde=False);\n",
    "len(clean_df.comment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the shorter sequences are padded and the longer truncated to ensure a uniform size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items with reduced size due to padding: 4596 \n"
     ]
    }
   ],
   "source": [
    "print( \"Number of items with reduced size due to padding: {} \".format(sum([ 1 if len(row_text) > 225 else 0 for row_text in encode_text])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_text = pad_sequences(encode_text, maxlen=225, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Split\n",
    "Here we will randomly split the comment data into Train/Test groups using 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(encode_text)) < 0.8\n",
    "train_x = encode_text[msk]\n",
    "train_y = encode_text_y[msk].to_numpy()\n",
    "\n",
    "test_x = encode_text[~msk]\n",
    "test_y = encode_text_y[~msk].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train / Test Sets\n",
    "This is incase something crashes, we can reload from here without re-running all the previous steps which take hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_x.pkl', 'wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open( 'train_y.pkl', 'wb') as f:\n",
    "    pickle.dump(train_y, f)\n",
    "    \n",
    "with open( 'test_x.pkl', 'wb') as f:\n",
    "    pickle.dump(test_x, f)\n",
    "    \n",
    "with open( 'test_y.pkl', 'wb') as f:\n",
    "    pickle.dump(test_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vectors = dict()\n",
    "# filename = 'glove.6B.50d.txt'\n",
    "# with  open(filename, encoding=\"utf-8\") as file:\n",
    "#     for l in file:\n",
    "#         val = l.split()\n",
    "#         word = val[0]\n",
    "#         vect = val[1:]\n",
    "#         glove_vectors[word] = list(map(float, vect))\n",
    "\n",
    "# vocab_length = len(glove_vectors)\n",
    "# vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print({k: glove_vectors[k] for k in sorted(glove_vectors.keys())[:5]})\n",
    "# print(len(ordered_glove_values))\n",
    "# print(len(glove_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# # ordered_glove_values = [glove_vectors[k] for k in token.word_index.keys()]\n",
    "# # ordered_glove_values[:5]\n",
    "# ordered_glove_values = []\n",
    "# def create_list(n):\n",
    "#     return [random.random() for i in range(n)]\n",
    "\n",
    "# for key in list(token.word_index.keys()):\n",
    "#     if key in glove_vectors:\n",
    "#         ordered_glove_values.append(glove_vectors[key])\n",
    "#     else:\n",
    "#         print(key)\n",
    "#         ordered_glove_values.append(create_list(50))\n",
    "        \n",
    "    \n",
    "\n",
    "# [len(row) for row in ordered_glove_values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Encode Attend Predict\n",
    "Here is the implementation for the Embed Encode Attend Predict (EEAP) framework.  Much of the code for this section was reporposed from a presentation produced by Martin Gorner of Google and is available on youtube at https://www.youtube.com/watch?v=pzOzmxCR37I&t=1506s\n",
    "\n",
    "In the examples below the network has been changed in some meaningful ways, specifically but not limited to a a larger RNN with dropout to prevent overfitting.  Different sizes for the RNN and attention layers, coupled with some optimizations for the learning rate and number of steps that is specific to this dataset.\n",
    "\n",
    "\n",
    "![alt text](eeap.png \"Embed Encode Attend Predict Graph\")\n",
    "\n",
    "Above you can see the network for the EEAP framework.\n",
    "- Embed: input is a the list of tokenized words and outputs a list of vectors\n",
    "- Encode: input word embeddings are processed through a bidirectional rnn\n",
    "- Attend: input is the vector output from the RNN.  This is feed into a small neural network.  This ends up learning attributes from the RNN and being able to add higher weights to items of significance.\n",
    "- Predict: The prediction step from the output of the attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(x):\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "        x, \n",
    "        vocab_size=vocab_size, \n",
    "        embed_dim=50)\n",
    "    \n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(w_vect):\n",
    "    LAYER_COUNT = 2\n",
    "    rnn_fw = [tf.contrib.rnn.GRUCell(128) for _ in range(LAYER_COUNT)]\n",
    "    rnn_bw = [tf.contrib.rnn.GRUCell(128) for _ in range(LAYER_COUNT)]\n",
    "    rnn_fw[:-1] = [tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5) for cell in rnn_fw[:-1]]\n",
    "    rnn_bw[:-1] = [tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5) for cell in rnn_bw[:-1]]\n",
    "    rnn_fw = tf.nn.rnn_cell.MultiRNNCell(rnn_fw, state_is_tuple=False)\n",
    "    rnn_bw = tf.nn.rnn_cell.MultiRNNCell(rnn_bw, state_is_tuple=False)\n",
    "    outputs,_= tf.nn.bidirectional_dynamic_rnn(rnn_fw, rnn_bw, w_vect,dtype=tf.float32,time_major=False)\n",
    "    outputs = tf.concat(outputs, axis = 2)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attend(alphas, attention_size, attention_depth):\n",
    "  \n",
    "    alphas = tf.concat(alphas, axis = 2)\n",
    "\n",
    "    inputs_shape = alphas.shape\n",
    "    sequence_length = inputs_shape[1].value\n",
    "    final_layer_size = inputs_shape[2].value\n",
    "\n",
    "    x = tf.reshape(alphas, [-1, final_layer_size])\n",
    "    for _ in range(attention_depth-1):\n",
    "        x = tf.layers.dense(tf.nn.dropout(x, 0.8), attention_size, activation = tf.nn.relu)\n",
    "    x = tf.layers.dense(x, 1, activation = None)\n",
    "    logits = tf.reshape(x, [-1, sequence_length, 1])\n",
    "    softmax = tf.nn.softmax(logits, dim = 1)\n",
    "\n",
    "    output = tf.reduce_sum(alphas * softmax, 1)\n",
    "\n",
    "    return output, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_spec_for_softmax_classification(\n",
    "    logits, labels, mode, alphas):\n",
    "    \"\"\"Returns EstimatorSpec instance for softmax classification.\"\"\"\n",
    "\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\n",
    "                'class': predicted_classes,\n",
    "                'prob': tf.nn.softmax(logits),\n",
    "                'attention': alphas\n",
    "            })\n",
    "\n",
    "    onehot_labels = tf.one_hot(labels, 2, 1, 0)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss, \n",
    "                                      global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, \n",
    "                                          loss=loss, \n",
    "                                          train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predicted_classes),\n",
    "      'auc': tf.metrics.auc(\n",
    "          labels=labels, predictions=predicted_classes),    \n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(encoding, labels, mode, alphas):\n",
    "    logits = tf.layers.dense(encoding, 2, activation=None)\n",
    "    return estimator_spec_for_softmax_classification(\n",
    "          logits=logits, labels=labels, mode=mode, alphas=alphas)\n",
    "\n",
    "def bi_rnn_model(features, labels, mode):\n",
    "    \"\"\"RNN model to predict from sequence of words to a class.\"\"\"\n",
    "\n",
    "    word_vectors = embed(features)\n",
    "    outputs = encode(word_vectors)\n",
    "    encoding, alphas = attend(outputs, \n",
    "                            128, \n",
    "                            4)\n",
    "\n",
    "    return predict(encoding, labels, mode, alphas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'checkpoints/1598133173', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f86feb1dc88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-37-42b8e638bdab>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-37-42b8e638bdab>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-37-42b8e638bdab>:9: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efda0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efda0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86fe982e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86fe982e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86fe982e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86fe982e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706e47048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706e47048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706e47048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706e47048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efef0>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86ff2efef0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706dc9dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706dc9dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706dc9dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f8706dc9dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86ff2efd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86ff2efd30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86ff2efd30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86ff2efd30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-38-bdea7c6ee631>:11: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-38-bdea7c6ee631>:11: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86ff2fc198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From <ipython-input-38-bdea7c6ee631>:14: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86fec4bcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86fec4bcf8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86fec4bcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86fec4bcf8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.68868214, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.17759\n",
      "INFO:tensorflow:loss = 0.18547533, step = 101 (563.105 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 104 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.164625\n",
      "INFO:tensorflow:loss = 0.106334716, step = 201 (607.461 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 202 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.1706\n",
      "INFO:tensorflow:loss = 0.11866012, step = 301 (586.140 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 306 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.189533\n",
      "INFO:tensorflow:loss = 0.10893221, step = 401 (527.642 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 421 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.207102\n",
      "INFO:tensorflow:loss = 0.0531011, step = 501 (482.796 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 545 into checkpoints/1598133173/model.ckpt.\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 0.20477\n",
      "INFO:tensorflow:loss = 0.06782959, step = 601 (488.351 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 671 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.191302\n",
      "INFO:tensorflow:loss = 0.06486878, step = 701 (522.816 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 780 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.192944\n",
      "INFO:tensorflow:loss = 0.0563256, step = 801 (518.215 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 892 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.184061\n",
      "INFO:tensorflow:loss = 0.03148599, step = 901 (543.286 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.200092\n",
      "INFO:tensorflow:loss = 0.025611341, step = 1001 (499.932 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1013 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1095 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.140866\n",
      "INFO:tensorflow:loss = 0.015951885, step = 1101 (709.784 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1182 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.142082\n",
      "INFO:tensorflow:loss = 0.020473193, step = 1201 (703.827 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1267 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.143412\n",
      "INFO:tensorflow:loss = 0.021395015, step = 1301 (697.296 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1357 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1400 into checkpoints/1598133173/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.021846466.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7f86feb1df60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "current_time = str(int(time.time()))\n",
    "model_dir = os.path.join('checkpoints',current_time)\n",
    "classifier = tf.estimator.Estimator(model_fn=bi_rnn_model, \n",
    "                                    model_dir=model_dir)\n",
    "\n",
    "\n",
    "# Train.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x=train_x,\n",
    "  y=train_y,\n",
    "  batch_size=512,\n",
    "  num_epochs=None,\n",
    "  shuffle=True)\n",
    "\n",
    "classifier.train(input_fn=train_input_fn, \n",
    "                 steps=1400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2320>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2320>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86feb1d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86feb1d710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86feb1d710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86feb1d710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2828>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1bf2828>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3d828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3dbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1c3dbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1bf2390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86febb0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86febb0390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86febb0390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86febb0390>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/1598133173/model.ckpt-1400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1b31a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1b31a20>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1b31a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1b31a20>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e4ed0208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e4ed0208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e4ed0208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e4ed0208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa0b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa0b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1795b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1795b00>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1795b00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f86e1795b00>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baaa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baaa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baaa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baaa90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x7f86e1baa518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e1754278>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e10b64e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e10b64e0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e10b64e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f86e10b64e0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /home/psagui/.local/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-08-22T20:13:55Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/1598133173/model.ckpt-1400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-08-22-20:15:39\n",
      "INFO:tensorflow:Saving dict for global step 1400: accuracy = 0.9539514, auc = 0.8639751, global_step = 1400, loss = 0.17883219\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1400: checkpoints/1598133173/model.ckpt-1400\n",
      "Accuracy: 0.953951\n",
      "AUC: 0.863975\n"
     ]
    }
   ],
   "source": [
    "# Predict.\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x=test_x,\n",
    "  y=test_y,\n",
    "  num_epochs=1,\n",
    "  shuffle=False)\n",
    "\n",
    "predictions = classifier.predict(input_fn=test_input_fn)\n",
    "\n",
    "y_predicted = []\n",
    "alphas_predicted = []\n",
    "for p in predictions:\n",
    "    y_predicted.append(p['class'])\n",
    "    alphas_predicted.append(p['attention'])\n",
    "\n",
    "scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "print('Accuracy: {0:f}'.format(scores['accuracy']))\n",
    "print('AUC: {0:f}'.format(scores['auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_token = {value:key for key, value in token.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def token_reverse(x):\n",
    "    txt_token = [rev_token[tok] if tok in rev_token else \"\" for tok in x]\n",
    "    return \" \".join(txt_token)\n",
    "test_x_rev_token = [token_reverse(tokens) for tokens in test_x ]\n",
    "solutions = pd.DataFrame({'predicted':y_predicted,'actual': test_y, 'text': test_x_rev_token  })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19549</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>my thoughts you are a fucking piece of shit ! die and burn in hell !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>okay blue tie , you freaking moron , everybody knows how he edits his stuff , just read the hour long article about his controverss and also , why should it be inlcuded ? to keep not as biased as it already is the bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7704</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>`` you are a moron plain and simple the movie mentions the word `` '' satan '' for comedy effect suddenly , according to you , everyone involved in the creation and production of the aqua teen hunger force movie is suddenly a real , practicing satanist by the way , the majority of what you just made up is bullshit watched the movie in theaters , and twice on the dvd i can safely say you have not the slightest idea of what you are babbling on about thanks for being a gigantic idiot ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>why not block that cunt samsara for being a pedantic , know nothing twat ? oh , i guess probably because you are one yourself and you guys have got to stick together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25668</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>and your little faggot boy propol , pray , pray little wonk that i do not find your little ass and kill you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ed poor are not no stinkin monkey ! user ed poor are not no stinkin monkey !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you can not delete shit from my talk page you faggot , get that shit out of here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>thats cos im a schitzophrenic toat but ure just an ignorant idiot ill be here long after you have got fed up and left # )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28972</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hey bitch you are the biggest faggot ever ! i know you like that cock in ypur ass and all but do you even feel it anymore or is it like throwing a hotdog down a hallway ? oh yeah by the way your aunt just called she says she wants you out of her basement and she wants you to get a real job you little bitch and one more thing i heard you weigh # pounds ive got a tip get active and eat subway you little bitch !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>why do you keep crying ( bitching ) about every revert ? #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stop accusing me of vandalism ( wanker ) gwernol you probably hate tony blair more than i do so leave me to dis the shit on him thanks , bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>vandalism of my user page please do not vandalize my user page i am also a bc resident and i am sick of all you damn potheads if i was in power round you all up and shoot you like to horrible animals you are make hitler stalin look like cartoon characters ! fuck you , you worthless peice of shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8559</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>the quote that you tried to preserve said that all americans have incest , the real quote does not get over it is in the history theres nothing to argue about you are an idiot you were wrong you got burned ! now go fuck your mother picture it ! your cousin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7409</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>go f yourself m fuker you are a modi ass fucker , homo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21267</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>your a fucking bitch faced fuck head asshole mother fucker shit breath bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13246</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hey man whats up ? so yeah cool i just wanted to make sure we are cool and all cus you deletin my page , that just dont flow feel me ? i hope we can be real good friends feel me ? laughing out loud jk fuck you but serious dont delete my page i got high up connections wit mista wikipedia so dont mess sincerely , ur mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27876</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>this article is terrible honestly , one of the worst philosophy articles on wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>do not screw me hey do not screw me more im doing contributions for wikipedia and you are screwing me , what is your problem ? leave me alone , you do not have your own life , like jenkins in south park leave me alone , you fag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24478</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>screw you , not scared of your little warnings , so retart , fuck you too !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22945</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>whats your problem ? will you please stop harassing me ? you are constantly attacking me and this is becoming extremely annoying !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  actual  \\\n",
       "19549          1       1   \n",
       "26137          1       1   \n",
       "7704           1       1   \n",
       "1352           1       1   \n",
       "25668          1       1   \n",
       "10251          1       1   \n",
       "6507           1       1   \n",
       "2955           1       1   \n",
       "28972          1       1   \n",
       "8757           1       1   \n",
       "27999          1       1   \n",
       "16046          1       1   \n",
       "8559           1       1   \n",
       "7409           1       1   \n",
       "21267          1       1   \n",
       "13246          1       1   \n",
       "27876          1       1   \n",
       "8236           1       1   \n",
       "24478          1       1   \n",
       "22945          1       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \n",
       "19549                                                                                                                                                                                                                                                                                                                                                      my thoughts you are a fucking piece of shit ! die and burn in hell !                                                                                                                                                                                                                   \n",
       "26137                                                                                                                                                                                                                             okay blue tie , you freaking moron , everybody knows how he edits his stuff , just read the hour long article about his controverss and also , why should it be inlcuded ? to keep not as biased as it already is the bird                                                                                                                                                                                      \n",
       "7704   `` you are a moron plain and simple the movie mentions the word `` '' satan '' for comedy effect suddenly , according to you , everyone involved in the creation and production of the aqua teen hunger force movie is suddenly a real , practicing satanist by the way , the majority of what you just made up is bullshit watched the movie in theaters , and twice on the dvd i can safely say you have not the slightest idea of what you are babbling on about thanks for being a gigantic idiot ``                                                                                                                                   \n",
       "1352                                                                                                                                                                                                                                                                       why not block that cunt samsara for being a pedantic , know nothing twat ? oh , i guess probably because you are one yourself and you guys have got to stick together                                                                                                                                                                                                  \n",
       "25668                                                                                                                                                                                                                                                                                                                      and your little faggot boy propol , pray , pray little wonk that i do not find your little ass and kill you                                                                                                                                                                                                            \n",
       "10251                                                                                                                                                                                                                                                                                                                                               ed poor are not no stinkin monkey ! user ed poor are not no stinkin monkey !                                                                                                                                                                                                                  \n",
       "6507                                                                                                                                                                                                                                                                                                                                             you can not delete shit from my talk page you faggot , get that shit out of here                                                                                                                                                                                                                 \n",
       "2955                                                                                                                                                                                                                                                                                                            thats cos im a schitzophrenic toat but ure just an ignorant idiot ill be here long after you have got fed up and left # )                                                                                                                                                                                                         \n",
       "28972                                                                         hey bitch you are the biggest faggot ever ! i know you like that cock in ypur ass and all but do you even feel it anymore or is it like throwing a hotdog down a hallway ? oh yeah by the way your aunt just called she says she wants you out of her basement and she wants you to get a real job you little bitch and one more thing i heard you weigh # pounds ive got a tip get active and eat subway you little bitch !                                                                                                                                        \n",
       "8757                                                                                                                                                                                                                                                                                                                                                              why do you keep crying ( bitching ) about every revert ? #                                                                                                                                                                                                                      \n",
       "27999                                                                                                                                                                                                                                                                                            stop accusing me of vandalism ( wanker ) gwernol you probably hate tony blair more than i do so leave me to dis the shit on him thanks , bob                                                                                                                                                                                                     \n",
       "16046                                                                                                                                                               vandalism of my user page please do not vandalize my user page i am also a bc resident and i am sick of all you damn potheads if i was in power round you all up and shoot you like to horrible animals you are make hitler stalin look like cartoon characters ! fuck you , you worthless peice of shit                                                                                                                                                                      \n",
       "8559                                                                                                                                                                                               the quote that you tried to preserve said that all americans have incest , the real quote does not get over it is in the history theres nothing to argue about you are an idiot you were wrong you got burned ! now go fuck your mother picture it ! your cousin                                                                                                                                                                               \n",
       "7409                                                                                                                                                                                                                                                                                                                                                                  go f yourself m fuker you are a modi ass fucker , homo                                                                                                                                                                                                                      \n",
       "21267                                                                                                                                                                                                                                                                                                                                           your a fucking bitch faced fuck head asshole mother fucker shit breath bitch                                                                                                                                                                                                                      \n",
       "13246                                                                                                                                              hey man whats up ? so yeah cool i just wanted to make sure we are cool and all cus you deletin my page , that just dont flow feel me ? i hope we can be real good friends feel me ? laughing out loud jk fuck you but serious dont delete my page i got high up connections wit mista wikipedia so dont mess sincerely , ur mother                                                                                                                                                             \n",
       "27876                                                                                                                                                                                                                                                                                                                                   this article is terrible honestly , one of the worst philosophy articles on wikipedia                                                                                                                                                                                                                     \n",
       "8236                                                                                                                                                                                                                         do not screw me hey do not screw me more im doing contributions for wikipedia and you are screwing me , what is your problem ? leave me alone , you do not have your own life , like jenkins in south park leave me alone , you fag                                                                                                                                                                                  \n",
       "24478                                                                                                                                                                                                                                                                                                                                                screw you , not scared of your little warnings , so retart , fuck you too !                                                                                                                                                                                                                  \n",
       "22945                                                                                                                                                                                                                                                                                               whats your problem ? will you please stop harassing me ? you are constantly attacking me and this is becoming extremely annoying !                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[(solutions.predicted == 1) & (solutions.actual==1)].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Type 2 Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>hey barefoot we pwnd you when you came on the guardian try rubbing your kidneys for a bit !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27285</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>table change it back to the list , is so stupid and takes longer to change and edit stuff here my vote is to change it back agrees # disagrees #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19218</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>`` off to a crap start '' meadowhall is an indoor shopping centre in the sheffield , south yorkshire , england `` '' above is the opening sentence as i found it sheffield is not the sheffield sorted this ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16764</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>`` comley stop accusing me of being a vandly , pak # s username is clearly racist and therefore i believe that he should be blocked as im sure you are aware pak is the abriviation of a commonly used racist term for people of asion or pakistani origin please look in the dictionary if you do not understand ( and see if you can find any good words to describe yourself ) i will not tollerate racism on wikipedia and you being a socilist communist `` '' er should not either ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29257</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stop as wiki is not a waabi and deobandi platform so do not try ur sick views here first collect correct information and be on siratal mustakeem ( t c )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11186</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this stub sucks sorry but this is a major museum , and i can not find it in wikipedia there are several images in wikipedia of vehicles which are in the museum , including the tank with david cerny painted pink in # ( one of those pictures is by me , but there are others )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>better change that to nerd man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26365</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>lack of new york post report in the artical lack of lavin new york post report in the artical there should be someting in the artical about lavin exposing olbermann about his alegedly being a pervert with a small penis audio here artical here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26771</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>all while real people are spilling real blood and dying real deaths in a real war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11922</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>neutralhomer hates black people ! and so does gogo dodo !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i was wondering the same thing for my user page either way , i do not even understand why i was blocked in the first place other than that some idiot named rhobite seems to have a hard time dealing with defeat and reality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>there is no information about # percent greeks in albania so you are a chauvinist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>oh , no apology required ! yes , my user page is complete nonsense well , i am english , a leftist , and male , but the fascism tripe is just pure , immature silliness one of the reasons i am interested in greek history is because of their left wing political history please accept a from me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29013</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no vandalism in tge recent edit history of this ip , if you do not mind i want to finish writing what i was writing , and i do not understand what was so offensive about mocking a creationist for using the dark matter argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25310</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>computer models are telling us something quite different from what nature seems to be telling us , '' said emanuel , whose views on hurricanes and global warming have been prominently cited by al gore and other promoters of climate change fear he told the new york times `` '' there are various possible the big increase in hurricane power over the past # years or so may not have much to do with global warming , or the models are simply not faithfully reproducing what nature is doing hard to know which to believe yet in # , a few weeks before hurricane katrina struck new orleans , emmanuel asserted in a paper that he had found statistical evidence linking rising hurricane energy and global warming his conversion is a very important new development in the climate debate , said morano , a top aide to climate change skeptic sen james inhofe , r okla '' first , # turned out to be the tipping point for global warming fears , and # appears to be the year of vindication for skeptics as many prominent scientists reverse their climate views , more and more skeptical scientists speak out , and new data debunks man made climate fears now another major scientist reconsiders his views on a significant aspect of man made climate fears ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>you are a liar , you are changing and stealing turkish history please stop what you doing regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>`` and finally , the only reason i created the article here on wikipedia , it is because i could link it to other wikipedia articles , since it was about an animation , band , video game , cd album and cd single but like i said before , screw this it is because of losers so called administrators like you aka `` '' vandalism fighter wannabe '' that wikipedia is full of nonsense articles with innacurate info pftt , can not bother with these weirdos adieu # ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15612</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>liar , liar , pants on fire seriously i looked at your contributions , the tennis portal pages and the tennis page and there has been no descussion what so ever please do not lie #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15562</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>help this historian now thinks that sanhedrin is closer to neturei karta than to kach he is also seeing a fusion of kach and neturei karta , since both oppose the state i tried to clarify it , giving some examples of the thought of nk and kach ( without naming either ) , and what is his conclusion the press releases of the sanhedrin so far are more like nk please , please , try to explain to him that i am right how do you explain jewish politics to somebody with the brain of a fish ? ( talk )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5341</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>is he really jewish ? my friend jamal said jesus was a jew how come he didnt get killed in the ovens then ? i do not think jamal knows anything so whats up ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  actual  \\\n",
       "9659           0       1   \n",
       "27285          0       1   \n",
       "19218          0       1   \n",
       "16764          0       1   \n",
       "29257          0       1   \n",
       "11186          0       1   \n",
       "20740          0       1   \n",
       "26365          0       1   \n",
       "26771          0       1   \n",
       "11922          0       1   \n",
       "6526           0       1   \n",
       "2988           0       1   \n",
       "5363           0       1   \n",
       "29013          0       1   \n",
       "25310          0       1   \n",
       "7069           0       1   \n",
       "2564           0       1   \n",
       "15612          0       1   \n",
       "15562          0       1   \n",
       "5341           0       1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text  \n",
       "9659                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   hey barefoot we pwnd you when you came on the guardian try rubbing your kidneys for a bit !                                                                                                                                                                                                                \n",
       "27285                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         table change it back to the list , is so stupid and takes longer to change and edit stuff here my vote is to change it back agrees # disagrees #                                                                                                                                                                                                    \n",
       "19218                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    `` off to a crap start '' meadowhall is an indoor shopping centre in the sheffield , south yorkshire , england `` '' above is the opening sentence as i found it sheffield is not the sheffield sorted this ``                                                                                                                                                                                           \n",
       "16764                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            `` comley stop accusing me of being a vandly , pak # s username is clearly racist and therefore i believe that he should be blocked as im sure you are aware pak is the abriviation of a commonly used racist term for people of asion or pakistani origin please look in the dictionary if you do not understand ( and see if you can find any good words to describe yourself ) i will not tollerate racism on wikipedia and you being a socilist communist `` '' er should not either ``                                                                                                                                      \n",
       "29257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 stop as wiki is not a waabi and deobandi platform so do not try ur sick views here first collect correct information and be on siratal mustakeem ( t c )                                                                                                                                                                                                    \n",
       "11186                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  this stub sucks sorry but this is a major museum , and i can not find it in wikipedia there are several images in wikipedia of vehicles which are in the museum , including the tank with david cerny painted pink in # ( one of those pictures is by me , but there are others )                                                                                                                                                                          \n",
       "20740                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  better change that to nerd man                                                                                                                                                                                                                             \n",
       "26365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    lack of new york post report in the artical lack of lavin new york post report in the artical there should be someting in the artical about lavin exposing olbermann about his alegedly being a pervert with a small penis audio here artical here                                                                                                                                                                                       \n",
       "26771                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         all while real people are spilling real blood and dying real deaths in a real war                                                                                                                                                                                                                   \n",
       "11922                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            neutralhomer hates black people ! and so does gogo dodo !                                                                                                                                                                                                                        \n",
       "6526                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i was wondering the same thing for my user page either way , i do not even understand why i was blocked in the first place other than that some idiot named rhobite seems to have a hard time dealing with defeat and reality                                                                                                                                                                                       \n",
       "2988                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         there is no information about # percent greeks in albania so you are a chauvinist                                                                                                                                                                                                                    \n",
       "5363                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   oh , no apology required ! yes , my user page is complete nonsense well , i am english , a leftist , and male , but the fascism tripe is just pure , immature silliness one of the reasons i am interested in greek history is because of their left wing political history please accept a from me                                                                                                                                                                        \n",
       "29013                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     no vandalism in tge recent edit history of this ip , if you do not mind i want to finish writing what i was writing , and i do not understand what was so offensive about mocking a creationist for using the dark matter argument                                                                                                                                                                                      \n",
       "25310  computer models are telling us something quite different from what nature seems to be telling us , '' said emanuel , whose views on hurricanes and global warming have been prominently cited by al gore and other promoters of climate change fear he told the new york times `` '' there are various possible the big increase in hurricane power over the past # years or so may not have much to do with global warming , or the models are simply not faithfully reproducing what nature is doing hard to know which to believe yet in # , a few weeks before hurricane katrina struck new orleans , emmanuel asserted in a paper that he had found statistical evidence linking rising hurricane energy and global warming his conversion is a very important new development in the climate debate , said morano , a top aide to climate change skeptic sen james inhofe , r okla '' first , # turned out to be the tipping point for global warming fears , and # appears to be the year of vindication for skeptics as many prominent scientists reverse their climate views , more and more skeptical scientists speak out , and new data debunks man made climate fears now another major scientist reconsiders his views on a significant aspect of man made climate fears ''  \n",
       "7069                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            you are a liar , you are changing and stealing turkish history please stop what you doing regards                                                                                                                                                                                                                 \n",
       "2564                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        `` and finally , the only reason i created the article here on wikipedia , it is because i could link it to other wikipedia articles , since it was about an animation , band , video game , cd album and cd single but like i said before , screw this it is because of losers so called administrators like you aka `` '' vandalism fighter wannabe '' that wikipedia is full of nonsense articles with innacurate info pftt , can not bother with these weirdos adieu # ``                                                                                                                                         \n",
       "15612                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          liar , liar , pants on fire seriously i looked at your contributions , the tennis portal pages and the tennis page and there has been no descussion what so ever please do not lie #                                                                                                                                                                                               \n",
       "15562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               help this historian now thinks that sanhedrin is closer to neturei karta than to kach he is also seeing a fusion of kach and neturei karta , since both oppose the state i tried to clarify it , giving some examples of the thought of nk and kach ( without naming either ) , and what is his conclusion the press releases of the sanhedrin so far are more like nk please , please , try to explain to him that i am right how do you explain jewish politics to somebody with the brain of a fish ? ( talk )                                                                                                                             \n",
       "5341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 is he really jewish ? my friend jamal said jesus was a jew how come he didnt get killed in the ovens then ? i do not think jamal knows anything so whats up ?                                                                                                                                                                                                "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[(solutions.predicted == 0) & (solutions.actual==1)].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are examples of comments the EEAP model predicted not toxic but wikipedia concluded was toxic.  Now I am not defending the above statements but several of these are terse or rude but not nessisarliy toxic.  Considering the labels are possibly done by humans you could have some errors on the actual categorization.  Some specific examples include:\n",
    "    - Item(11654): man let me edit the damn pink spiders page\n",
    "    - Item(16829) : revert and ask for this page to be protected from vandals like you\n",
    "    - Item(10059) : im glad you have gone do not come back\n",
    "    - Item(14825) : collapse hide text bunch of uncivil nonsense\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Type 1 Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11600</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dude place the following in the large text box please stop raping little boys in the anus do not you know that is illegal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12214</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>blunderson , i was talking to you looking at the afghanistan talk page you have been collaborating with everyone of nisarkands now blocked socks either you are both afghan nationalists with afghan nationalist povs or you are online boyfriends which is it blunderson ? ps looser , do not criticize my spelling not a looser like you trying to have perfect spelling when talking to morons like you in all the articles written on wikipedia though my spelling is perfect you on the other hand have contributed nothing to wikipedia except edit warring and many blunders i bet you are a real dumbass in real life and you just use spell check when writing online after all , you have a degree in the easiest major on any campus theology ? your degree is a joke and useless and you are a joke and useless to wikipedia pps by the way stop trying to become an admin asshole , you have a block log and you will never become an admin with that type of record or you can keep wasting your time since you have nothing better to do than to be here # you must smell like a sock by now since sure you have not showered for weeks since you are here # trying to become an admin which will never happen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>i would also like to ask , do you regularly spend your time on heidegger and other philosophy articles , or have you only proceeded because of your harassment of me ? i am in the process of improving this article as we speak ( and as you waste one of your betters time with your childish games ) what precisely are you doing ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19884</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you guys do realise that you are not going to stop people enforcing honesty over your sad little corner of the internet , right ? whilst you are pulling your hair out that honest folks like me exist , we are in your wikipedia , correcting your dishonest edits it kills you , i know that , but it is time for the lies to stop , guise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` now i get your problem you really do not understand what you are being told , so when you call someone an `` '' ignorant ass '' you think that your own self was just mocked first , did i call it confusing ? no i said that you just mentioned it for some reason with no relation to the way it was told or the situation , and i said that it was a reply to you , saying that vandalizing , as you accused me when reverting my contributions , by writing `` '' cum , leave '' makes no sense and your `` '' wp status '' is relevant as i want to know who the hell are you , and what do you want and think when you keep submitting blocks to my user , as well for keeping reverting my talk page when trying to get it clear i do not need this `` '' discussion '' and i was not talking to you and not an `` '' editor '' , all i did was improving that article seen , and that is why you are even more annoying and less helpful ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` the problem is that there already is another `` '' planet x '' out there in the media , one which is predicted by delusional freaks who think they have chips in their heads to collide with earth in # that planet x has its own article , zetatalk , so having one article for lowells planet x , one article for other objects called planet x , and another for the loony planet x seems impractical podous ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24477</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` never call him nazi o talk ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12958</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir have no life , you sped all your time editing on wikipedia ya i bet that gets you lots of chicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28817</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` i love you jeeny has smiled at you ! smiles promote wikilove and hopefully this one has made your day better spread the wikilove by smiling at someone else , whether it be someone you have had disagreements with in the past or a good friend happy editing ! smile at others by adding to their talk page with a friendly message thank you so much for everything you are great ! because you are related to me , in some weird way ) plus you are a very valuable wikpedian ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23389</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>wow dude you are a real douche wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16595</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>just leave us alone if you keep harassing people , you will be blocked like cj dub was and i do not have to check your phoney block log since i come here everyday and am able to edit wiki pages with no problem but i did see a page that stated that cj dub was blocked just because people come from europe or other places or because they do not like you or your politics , it does not mean you should hate them you can not control people or their feelings towards you wiki is a place where people contribute their time and energy to enlighten the world , not for one nation or culture only wiki should not be read from an american point of view i come here to contribute to and correct articles because many times articles state inaccuracies that must be corrected this is done because much of the articles are written from a point of view that is one sided from the american experience not all articles are one sided , though but the ones that are , often time does not represent the whole truth therefore , other voices from elsewhere are encouraged to contribute and correct them we welcome these other voices and never forget wiki is for all voices , not for only one voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21560</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks for unblocking me , and yeah i understand that someone else would of blocked me , and i was acting like an idiot yesterday , because of everything that lead to what happened today started yesterday so been mad , and sure i was bothering you and everyone else , so sorry i feel kind of stupid but eh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11858</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>it is good that you do not understand , if you understand , oversit laughing out loud kodekat talk should i okay keep it simple, stupid you )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15374</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` yeah , to `` '' list of japanese words known by american nerds ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26820</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>southbridge proprietary bus northbridge cpu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29767</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>a never gon na give you up never gon na let you down never gon na run around and desert you never gon na make you cry never gon na say goodbye never gon na tell a lie and hurt you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fully cited on indo iranians i am not into duplicating discussions onto pages where they are completely offtopic , and am in favour of removing the entire pointless section ( )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` pete , what makes you think you can go around and spread insults and personal attacks ? what makes you think that you can call me `` '' arrogant '' and `` '' rude '' or kick me while down ? i would report you for this ongoing behavior of yours if only i was not blocked but maybe someone will see my page and do just that further , my `` '' recruiting '' of other editors is irrelevant if you read that post of mine on yahoo , you would see that i told everyone that , regardless of their position , appreciate their presence i was not trying to convince other people to come over and post with my pov in mind the fact that someone did come over and post my pov was completely beyond my control finally , i did discuss these edits i discussed them on the talk page for a week what all four editors were against was not the edit i did make , but the original proposal which was for deleting the entire section i , however , preserved the most critical points of that paragraph and moved it to `` '' reception of steiner '' to get rid of the problem of `` '' undue weight `` '' ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12356</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>`` facts are facts , doc james ia a liar period ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5113</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>state your user name a hole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  actual  \\\n",
       "11600          1       0   \n",
       "12214          1       0   \n",
       "5881           1       0   \n",
       "19884          1       0   \n",
       "9504           1       0   \n",
       "19681          1       0   \n",
       "24477          1       0   \n",
       "12958          1       0   \n",
       "28817          1       0   \n",
       "23389          1       0   \n",
       "16595          1       0   \n",
       "21560          1       0   \n",
       "11858          1       0   \n",
       "15374          1       0   \n",
       "26820          1       0   \n",
       "29767          1       0   \n",
       "26115          1       0   \n",
       "278            1       0   \n",
       "12356          1       0   \n",
       "5113           1       0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \n",
       "11600                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            dude place the following in the large text box please stop raping little boys in the anus do not you know that is illegal ?                                                                                                                                                                                                          \n",
       "12214  blunderson , i was talking to you looking at the afghanistan talk page you have been collaborating with everyone of nisarkands now blocked socks either you are both afghan nationalists with afghan nationalist povs or you are online boyfriends which is it blunderson ? ps looser , do not criticize my spelling not a looser like you trying to have perfect spelling when talking to morons like you in all the articles written on wikipedia though my spelling is perfect you on the other hand have contributed nothing to wikipedia except edit warring and many blunders i bet you are a real dumbass in real life and you just use spell check when writing online after all , you have a degree in the easiest major on any campus theology ? your degree is a joke and useless and you are a joke and useless to wikipedia pps by the way stop trying to become an admin asshole , you have a block log and you will never become an admin with that type of record or you can keep wasting your time since you have nothing better to do than to be here # you must smell like a sock by now since sure you have not showered for weeks since you are here # trying to become an admin which will never happen  \n",
       "5881                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i would also like to ask , do you regularly spend your time on heidegger and other philosophy articles , or have you only proceeded because of your harassment of me ? i am in the process of improving this article as we speak ( and as you waste one of your betters time with your childish games ) what precisely are you doing ?                                                                                                                                                                  \n",
       "19884                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      you guys do realise that you are not going to stop people enforcing honesty over your sad little corner of the internet , right ? whilst you are pulling your hair out that honest folks like me exist , we are in your wikipedia , correcting your dishonest edits it kills you , i know that , but it is time for the lies to stop , guise                                                                                                                                                               \n",
       "9504                                                                                                                                                                                                                                                 `` now i get your problem you really do not understand what you are being told , so when you call someone an `` '' ignorant ass '' you think that your own self was just mocked first , did i call it confusing ? no i said that you just mentioned it for some reason with no relation to the way it was told or the situation , and i said that it was a reply to you , saying that vandalizing , as you accused me when reverting my contributions , by writing `` '' cum , leave '' makes no sense and your `` '' wp status '' is relevant as i want to know who the hell are you , and what do you want and think when you keep submitting blocks to my user , as well for keeping reverting my talk page when trying to get it clear i do not need this `` '' discussion '' and i was not talking to you and not an `` '' editor '' , all i did was improving that article seen , and that is why you are even more annoying and less helpful ``                           \n",
       "19681                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         `` the problem is that there already is another `` '' planet x '' out there in the media , one which is predicted by delusional freaks who think they have chips in their heads to collide with earth in # that planet x has its own article , zetatalk , so having one article for lowells planet x , one article for other objects called planet x , and another for the loony planet x seems impractical podous ``                                                                                                                                                   \n",
       "24477                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      `` never call him nazi o talk ``                                                                                                                                                                                                                           \n",
       "12958                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             you sir have no life , you sped all your time editing on wikipedia ya i bet that gets you lots of chicks                                                                                                                                                                                                            \n",
       "28817                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    `` i love you jeeny has smiled at you ! smiles promote wikilove and hopefully this one has made your day better spread the wikilove by smiling at someone else , whether it be someone you have had disagreements with in the past or a good friend happy editing ! smile at others by adding to their talk page with a friendly message thank you so much for everything you are great ! because you are related to me , in some weird way ) plus you are a very valuable wikpedian ``                                                                                                                                      \n",
       "23389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    wow dude you are a real douche wow                                                                                                                                                                                                                           \n",
       "16595        just leave us alone if you keep harassing people , you will be blocked like cj dub was and i do not have to check your phoney block log since i come here everyday and am able to edit wiki pages with no problem but i did see a page that stated that cj dub was blocked just because people come from europe or other places or because they do not like you or your politics , it does not mean you should hate them you can not control people or their feelings towards you wiki is a place where people contribute their time and energy to enlighten the world , not for one nation or culture only wiki should not be read from an american point of view i come here to contribute to and correct articles because many times articles state inaccuracies that must be corrected this is done because much of the articles are written from a point of view that is one sided from the american experience not all articles are one sided , though but the ones that are , often time does not represent the whole truth therefore , other voices from elsewhere are encouraged to contribute and correct them we welcome these other voices and never forget wiki is for all voices , not for only one voice  \n",
       "21560                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         thanks for unblocking me , and yeah i understand that someone else would of blocked me , and i was acting like an idiot yesterday , because of everything that lead to what happened today started yesterday so been mad , and sure i was bothering you and everyone else , so sorry i feel kind of stupid but eh                                                                                                                                                                       \n",
       "11858                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             it is good that you do not understand , if you understand , oversit laughing out loud kodekat talk should i okay keep it simple, stupid you )                                                                                                                                                                                                       \n",
       "15374                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         `` yeah , to `` '' list of japanese words known by american nerds ''                                                                                                                                                                                                                    \n",
       "26820                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        southbridge proprietary bus northbridge cpu                                                                                                                                                                                                                              \n",
       "29767                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    a never gon na give you up never gon na let you down never gon na run around and desert you never gon na make you cry never gon na say goodbye never gon na tell a lie and hurt you                                                                                                                                                                                          \n",
       "26115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             fully cited on indo iranians i am not into duplicating discussions onto pages where they are completely offtopic , and am in favour of removing the entire pointless section ( )                                                                                                                                                                                                    \n",
       "278                                                                                                   `` pete , what makes you think you can go around and spread insults and personal attacks ? what makes you think that you can call me `` '' arrogant '' and `` '' rude '' or kick me while down ? i would report you for this ongoing behavior of yours if only i was not blocked but maybe someone will see my page and do just that further , my `` '' recruiting '' of other editors is irrelevant if you read that post of mine on yahoo , you would see that i told everyone that , regardless of their position , appreciate their presence i was not trying to convince other people to come over and post with my pov in mind the fact that someone did come over and post my pov was completely beyond my control finally , i did discuss these edits i discussed them on the talk page for a week what all four editors were against was not the edit i did make , but the original proposal which was for deleting the entire section i , however , preserved the most critical points of that paragraph and moved it to `` '' reception of steiner '' to get rid of the problem of `` '' undue weight `` '' ``       \n",
       "12356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        `` facts are facts , doc james ia a liar period ``                                                                                                                                                                                                                       \n",
       "5113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          state your user name a hole                                                                                                                                                                                                                             "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[(solutions.predicted == 1) & (solutions.actual==0)].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see some examples of items that clearly the model categorized incorrectly but some of these although techinically incorrect might have been a data collection error.\n",
    "\n",
    "Examples:\n",
    "- Item(14509) : what the fuck tce\n",
    "- Item(29931) : *** text not included ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample True Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` role in the fall of communism i have just added a new section on john paul iis role in the fall of communism i realized afterward that there is mention of this topic in the `` '' relationship with dictatorships '' section however , i think that the topic deserves more than a few sentences tucked away in a section with an unreable title such as `` '' relationship with dictatorships '' ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ps and world of warcraft and city of heroes city of villains both use it not sure if coh actually keeps track of healing i think only radius heals , if that meaning that if wow does do all heals , describing it in those solid terms will make the article less correct in reference to cohs aggro i also believe that coh also ramps up the hate if a particular creature is weak against a particular attack type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22329</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this is a question since it is not easily dissolved in water , what is kcal absorbed by a human body from wheat bran ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>feel free to block me longer i guess i lost my interest for the project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22910</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` more errors in earlier comments # `` '' batista was known for having his army mutilate opponents and display their dead bodies on television '' this is simply not true , though quaint if compared to gw bush # `` '' pretty much a toss up '' the crimes ( and stupidity ) of castro , guevara and associates has been much greater and longer lasting , destroying the society and economy # since messrs grau and prio were duly elected , they would normally be and were recognized , `` '' despite their opposition to batista '' is a non sequitur ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8087</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rewrite relation to modern climate issues i think this section has to be rewritten taken alone , a niave reader may take this section to mean that this issue creates significant and credible evidence against anthropogenic global warming this is wp undue i suggest rewording there are credible arguments both for and against such a hypothesis to while there have been acadmic arguments both for and against such a hypothesis , the debate has had little impact on the general scientific consensus for man made gobal warming objections ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10619</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>as this is all my own opinion or surmise none of it is appropriate for the article , i may be completely wrong and giogn to a reliable source such as ioc publications on the subject would still be needed #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25752</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>your skin hello , sorry if im bothering you , but could you show me how to use your skin on my homepage ? all help and assistance would be gratefully recieved , thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28707</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>on the scales that you re talking about fewer than a hundred unique sites put together ( # vs # a ratio of # , not # ) , both forms are so rare as to make such comparisons almost irrelevant for this hypothetical reader who wouldn t be able to figure out what ecumenopoleis means from the context , it is given as a plural form at the beginning of the article it is patronising in the extreme to assume that using the etymologically correct plural will cause such difficulties particularly as all readers need do if they are confused is open the ecumenopoleis article in a new tab window to be told what it means in the first sentence the last remaining argument is sthetic it is undeniable that a ending sounds a lot better , and is far easier to say , than a ending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` regarding `` '' wars argument '' thanks , i will do that soon here is my thinking , krugmans criticism is mentioned # times in a article it is intellectual dishonesty ( and one of character ) to loudly criticize one school of thought and than refuse to even debate for an hour with an actual austrian economist where he has debated with politicians `` '' to boost his book sales '' in his own words so if his critic is in any way or form relevant , why not settle it once for all and end with austrians , or if he does not want to lose his time on irrelevant things , he has humanitarian incentive ( or why call out austrians in a first place ? ) so all in all , it comes to this , either criticizer and critic is irrelevant and should be deleted , or what is in my opinion far better solution , this discrepancy should be there for readers to know and de for themselves this could be done in one or two sentences , just a note that he refuses the offer of debate from # although # fund for food bank new york was established a link to site ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17780</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>userboxes i think that the actions of this user in deleting a large portion of userboxes without following process have seriously undermined the wiki way please be aware that there is a process for deletion , and that it does not involve unilateral action with no warning being bold is one thing , being destructive of the wiki way is another thing entirely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` i know its `` '' ok '' , you have no choice in the matter ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` drmies , almost afraid to ask who you are rooting for phightins ! ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks for the heads up ! i will take a look !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>trying to prove your point , for example , by indicating an article as if it should be taken for sooth , in an encyclopedia you openly proclaim `` '' racist , '' not to mention published before world war i , is ridiculous in the real definition of that word worthy of ridicule the same with referencing historical commentaries long removed from actual events , to prove egyptians were black men , when we have real egyptian records and artifacts to examine herodotus a reliable source on eighteenth dynasty egypt ? hello i can not say i understand the impulse egypt was a repetative place always the same sculptures , always the same rituals , always the same supplications , always the same monuments to egos in stone its colonnades inspired later architects in greece and rome , but for all its monumentality egyptian architecture was not worth much more than that is it because you suppose egypt was the first real civilization ? look up ancient sumer all the while the heart of the world is africa we evolved there as a species erectus to sapiens after that , we diverged biodiversity environmental adaptation and now we are a little differently hued standing next to each other and of slightly different skeleton why should that not make us brothers ? ( ) ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` notice reply the bottom line recapitulation hi i believe you used your administrator rights improperly since you accused me of multiple things i consider the content of the block message very offensive and untrue since the edit war technically did not occur ( one false accusation ) then calling non abusive ad hominem reactions as `` '' offensive personal attacks '' `` '' harassment '' ( second false accusation ) i realistically admit that policies of wikipedia sometimes can be absurd ( like for example using specific descriptive words for a young person will result in prohibition from contributing to wikipedia , since everything even agf responses are considered as extreme personal attacks ) , therefore we will not discuss the latter issue multiple references , like this one , supports my kylie minogues revert and the user nickyp # is notified now to not revert other peoples edits over personal or claims such as `` '' nu disco no disco , nu disco is different than disco '' but you clearly did not want to hear my opinion and probably just let the an i `` '' plebs '' de my `` '' destiny '' thank you for listening ``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>there is no need for personal attacks fennessy is known for disruptive edit warring and should not be attacking other users for disagreeing with him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the agreed version ( pending revision through further discussion and consensus ) , as my efforts to undo the reverts of people who refuse to make any kind of effort to discuss their objections on talk continually ( well , twice , prior to this # day thing , which does not count on this point ) result in me getting blocked thus , as i said on talk , going to stick with helping ling nut get his rewrite ready i hope admins will continue to monitor the six day war article and protect it so that versions which have had gone through extensive discussion and consensus forming processes are not simply reverted by these guys who refuse to discuss their objections suggestions , etc if you can do anything to help out the majority of editors who are reasonably working together to improve the article in this regard , i would appreciate it and it is probably fair for me to speak for that majority in that regard i trust my sion to let other editors whom i discuss the wording with actually implement the agreed version ( s ) will demonstrate my good faith to admins responsible for twice blocking me i just want to see this article improved , including by keeping it in compliance with wp npov thanks again cheers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i made pleny of good edits ! so stop saying that ! also , when will i be able to edit as i please ? i have seen many pages that need editing , but i can not so they still contain wrong information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>`` turkish liberation war could someone please revert this edit as it is adding material to the caption of an expansionist war on `` '' the losses of the invader instead of the invaded '' , making the article even more biassed then it already was , to the detriment of the occupied country , that has done nothing other than defending its territory i want to believe that the administrator who blocked me ( and other administrators ) for `` '' edit wars '' are following the activities of the editor who accused me of the edit war this is a very good example of how i was forced to an edit war , considering that one can not make a war alone there are always two parties in a war ``</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  actual  \\\n",
       "29608          0       0   \n",
       "7399           0       0   \n",
       "22329          0       0   \n",
       "6398           0       0   \n",
       "22910          0       0   \n",
       "8087           0       0   \n",
       "10619          0       0   \n",
       "25752          0       0   \n",
       "28707          0       0   \n",
       "17212          0       0   \n",
       "17780          0       0   \n",
       "21046          0       0   \n",
       "18266          0       0   \n",
       "1387           0       0   \n",
       "21357          0       0   \n",
       "5706           0       0   \n",
       "1823           0       0   \n",
       "1650           0       0   \n",
       "7561           0       0   \n",
       "19598          0       0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \n",
       "29608                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  `` role in the fall of communism i have just added a new section on john paul iis role in the fall of communism i realized afterward that there is mention of this topic in the `` '' relationship with dictatorships '' section however , i think that the topic deserves more than a few sentences tucked away in a section with an unreable title such as `` '' relationship with dictatorships '' ``                                                                                                                                                         \n",
       "7399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ps and world of warcraft and city of heroes city of villains both use it not sure if coh actually keeps track of healing i think only radius heals , if that meaning that if wow does do all heals , describing it in those solid terms will make the article less correct in reference to cohs aggro i also believe that coh also ramps up the hate if a particular creature is weak against a particular attack type                                                                                                                                                    \n",
       "22329                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   this is a question since it is not easily dissolved in water , what is kcal absorbed by a human body from wheat bran ?                                                                                                                                                                                                          \n",
       "6398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         feel free to block me longer i guess i lost my interest for the project                                                                                                                                                                                                                    \n",
       "22910                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        `` more errors in earlier comments # `` '' batista was known for having his army mutilate opponents and display their dead bodies on television '' this is simply not true , though quaint if compared to gw bush # `` '' pretty much a toss up '' the crimes ( and stupidity ) of castro , guevara and associates has been much greater and longer lasting , destroying the society and economy # since messrs grau and prio were duly elected , they would normally be and were recognized , `` '' despite their opposition to batista '' is a non sequitur ''                                                                                                                           \n",
       "8087                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    rewrite relation to modern climate issues i think this section has to be rewritten taken alone , a niave reader may take this section to mean that this issue creates significant and credible evidence against anthropogenic global warming this is wp undue i suggest rewording there are credible arguments both for and against such a hypothesis to while there have been acadmic arguments both for and against such a hypothesis , the debate has had little impact on the general scientific consensus for man made gobal warming objections ?                                                                                                                                          \n",
       "10619                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            as this is all my own opinion or surmise none of it is appropriate for the article , i may be completely wrong and giogn to a reliable source such as ioc publications on the subject would still be needed #                                                                                                                                                                                          \n",
       "25752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         your skin hello , sorry if im bothering you , but could you show me how to use your skin on my homepage ? all help and assistance would be gratefully recieved , thank you                                                                                                                                                                                                \n",
       "28707                                                                                                                                                                                                                                                                                                                                                                                                                                      on the scales that you re talking about fewer than a hundred unique sites put together ( # vs # a ratio of # , not # ) , both forms are so rare as to make such comparisons almost irrelevant for this hypothetical reader who wouldn t be able to figure out what ecumenopoleis means from the context , it is given as a plural form at the beginning of the article it is patronising in the extreme to assume that using the etymologically correct plural will cause such difficulties particularly as all readers need do if they are confused is open the ecumenopoleis article in a new tab window to be told what it means in the first sentence the last remaining argument is sthetic it is undeniable that a ending sounds a lot better , and is far easier to say , than a ending                                                                               \n",
       "17212                                                                                                                                                                                                              `` regarding `` '' wars argument '' thanks , i will do that soon here is my thinking , krugmans criticism is mentioned # times in a article it is intellectual dishonesty ( and one of character ) to loudly criticize one school of thought and than refuse to even debate for an hour with an actual austrian economist where he has debated with politicians `` '' to boost his book sales '' in his own words so if his critic is in any way or form relevant , why not settle it once for all and end with austrians , or if he does not want to lose his time on irrelevant things , he has humanitarian incentive ( or why call out austrians in a first place ? ) so all in all , it comes to this , either criticizer and critic is irrelevant and should be deleted , or what is in my opinion far better solution , this discrepancy should be there for readers to know and de for themselves this could be done in one or two sentences , just a note that he refuses the offer of debate from # although # fund for food bank new york was established a link to site ''               \n",
       "17780                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          userboxes i think that the actions of this user in deleting a large portion of userboxes without following process have seriously undermined the wiki way please be aware that there is a process for deletion , and that it does not involve unilateral action with no warning being bold is one thing , being destructive of the wiki way is another thing entirely                                                                                                                                                                    \n",
       "21046                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  `` i know its `` '' ok '' , you have no choice in the matter ``                                                                                                                                                                                                                  \n",
       "18266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        `` drmies , almost afraid to ask who you are rooting for phightins ! ``                                                                                                                                                                                                                    \n",
       "1387                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               thanks for the heads up ! i will take a look !                                                                                                                                                                                                                       \n",
       "21357  trying to prove your point , for example , by indicating an article as if it should be taken for sooth , in an encyclopedia you openly proclaim `` '' racist , '' not to mention published before world war i , is ridiculous in the real definition of that word worthy of ridicule the same with referencing historical commentaries long removed from actual events , to prove egyptians were black men , when we have real egyptian records and artifacts to examine herodotus a reliable source on eighteenth dynasty egypt ? hello i can not say i understand the impulse egypt was a repetative place always the same sculptures , always the same rituals , always the same supplications , always the same monuments to egos in stone its colonnades inspired later architects in greece and rome , but for all its monumentality egyptian architecture was not worth much more than that is it because you suppose egypt was the first real civilization ? look up ancient sumer all the while the heart of the world is africa we evolved there as a species erectus to sapiens after that , we diverged biodiversity environmental adaptation and now we are a little differently hued standing next to each other and of slightly different skeleton why should that not make us brothers ? ( ) ''  \n",
       "5706                                                                                                             `` notice reply the bottom line recapitulation hi i believe you used your administrator rights improperly since you accused me of multiple things i consider the content of the block message very offensive and untrue since the edit war technically did not occur ( one false accusation ) then calling non abusive ad hominem reactions as `` '' offensive personal attacks '' `` '' harassment '' ( second false accusation ) i realistically admit that policies of wikipedia sometimes can be absurd ( like for example using specific descriptive words for a young person will result in prohibition from contributing to wikipedia , since everything even agf responses are considered as extreme personal attacks ) , therefore we will not discuss the latter issue multiple references , like this one , supports my kylie minogues revert and the user nickyp # is notified now to not revert other peoples edits over personal or claims such as `` '' nu disco no disco , nu disco is different than disco '' but you clearly did not want to hear my opinion and probably just let the an i `` '' plebs '' de my `` '' destiny '' thank you for listening ``                         \n",
       "1823                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      there is no need for personal attacks fennessy is known for disruptive edit warring and should not be attacking other users for disagreeing with him                                                                                                                                                                                                          \n",
       "1650                                                      the agreed version ( pending revision through further discussion and consensus ) , as my efforts to undo the reverts of people who refuse to make any kind of effort to discuss their objections on talk continually ( well , twice , prior to this # day thing , which does not count on this point ) result in me getting blocked thus , as i said on talk , going to stick with helping ling nut get his rewrite ready i hope admins will continue to monitor the six day war article and protect it so that versions which have had gone through extensive discussion and consensus forming processes are not simply reverted by these guys who refuse to discuss their objections suggestions , etc if you can do anything to help out the majority of editors who are reasonably working together to improve the article in this regard , i would appreciate it and it is probably fair for me to speak for that majority in that regard i trust my sion to let other editors whom i discuss the wording with actually implement the agreed version ( s ) will demonstrate my good faith to admins responsible for twice blocking me i just want to see this article improved , including by keeping it in compliance with wp npov thanks again cheers  \n",
       "7561                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         i made pleny of good edits ! so stop saying that ! also , when will i be able to edit as i please ? i have seen many pages that need editing , but i can not so they still contain wrong information                                                                                                                                                                                       \n",
       "19598                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           `` turkish liberation war could someone please revert this edit as it is adding material to the caption of an expansionist war on `` '' the losses of the invader instead of the invaded '' , making the article even more biassed then it already was , to the detriment of the occupied country , that has done nothing other than defending its territory i want to believe that the administrator who blocked me ( and other administrators ) for `` '' edit wars '' are following the activities of the editor who accused me of the edit war this is a very good example of how i was forced to an edit war , considering that one can not make a war alone there are always two parties in a war ``                                                                                              "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[(solutions.predicted == 0) & (solutions.actual==0)].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='predicted', ylabel='actual'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEGCAYAAACEgjUUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdKUlEQVR4nO3de5xVVf3/8dd7ZkBJQEa5iIDihTQ0RSW1zPKSgKbhJf1ppWQUffOSmvoFryBq4rUyy0Il8YrklW+iSIiQFxREQgTUETFBBBRU5CIMfH5/nM14pGE4gzNz5pz9fvpYjzln7ctZW/E9i7XXWVsRgZmZFbeSfDfAzMzqn8PezCwFHPZmZingsDczSwGHvZlZCpTluwEb02yfszxNyP7L0sm35LsJ1ghtWYa+7DlqkzkrX7nlS39eQ3PP3swsBRptz97MrEGpuPu+DnszM4CS0ny3oF457M3MAFRww/C14rA3MwMP45iZpYJ79mZmKeCevZlZCrhnb2aWAp6NY2aWAh7GMTNLAQ/jmJmlgHv2ZmYpUORhX9xXZ2aWq9LS3EsNJHWSNF7STEmvSTonqR8kab6kaUk5KuuYiyRVSHpdUs+s+l5JXYWkAVn1O0l6Mal/QFLTTV2ew97MDDJj9rmWmlUC50dEV+BA4ExJXZNtv4uIbkkZnflYdQVOBvYAegF/llQqqRT4E3Ak0BU4Jes81ybn2hVYCvTdVKMc9mZmkBnGybXUICIWRMTU5PUyYBbQoYZDegMjIuKziHgbqAD2T0pFRMyJiNXACKC3JAGHAQ8mxw8Hjt3U5TnszcygVj17Sf0kTckq/ao/pToD+wAvJlVnSZouaZik8qSuA/Bu1mHzkrqN1W8LfBQRlRvU18hhb2YGterZR8TQiOieVYb+1+mk5sBDwLkR8QlwK7AL0A1YANzYkJfn2ThmZlCn8+wlNSET9PdGxMMAEbEwa/ttwD+St/OBTlmHd0zq2Ej9h0ArSWVJ7z57/41yz97MDDLLJeRaapCMqd8BzIqIm7Lq22ftdhwwI3k9CjhZ0haSdgK6AC8Bk4EuycybpmRu4o6KiADGAz9Mju8DPLapy3PP3swM6nKe/UHAqcCrkqYldReTmU3TDQhgLvBLgIh4TdJIYCaZmTxnRsRaAElnAWOAUmBYRLyWnK8/MELSVcArZH651Mhhb2YGdTaMExHPAtWdbHQNx1wNXF1N/ejqjouIOWRm6+TMYW9mBkX/DVqHvZkZOOzNzFLB69mbmaWAlzg2M0sBD+OYmaWAe/ZmZsVPDnszs+LnsDczSwGVOOzNzIqee/ZmZingsDczSwGHvZlZGhR31jvszczAPXszs1QoKfE3aM3Mip579mZmaVDcWe+wNzMD9+zNzFLBYW9mlgJeLsHMLAXcszczSwGHvZlZCjjszcxSwGFvZpYGxZ31DnszM/ByCWZmqeBhHDOzNCjurHfYN5SO7Vpx+5Wn0XbbFkTAsIee40/3P8PdQ06nS+d2ALRq0YyPlq3kwJOHAHDBz3rw097fZO26dZx/3YP884VZbNG0jH/ecS5Nm5ZRVlrKI/98hav+MhqAoVf8hIP325WPP10FQL/L72b6G/Pzc8H2pX3yySdccfmlVFS8gSSuuPK3bLHFllw1eCCrP/uM0rJSLr50EF/fay8mv/Qi5559Bh06dATgsO8dwf+ccVaer6CwuGdvdaJy7ToG3PQw02bPo/lXtuD5+/oz7sXZnDrgb1X7DPnNcXz86UoAdt95O07suS/7/vBq2rfZmtF/OYuvHzuYz1ZX0qvfzSxfuZqyshKeHvYbnnpuJi+9OheAi3//KI/8c1oertDq2nXXXM1B3z6YG39/M2tWr2blqlVceP65/M8ZZ/Ltg7/LvyZO4Pc3Xc8dd94NwD77deeWP/81z60uXMUe9vV2R0LS7pL6S7o5Kf0lfa2+Pq+xe/+DT5g2ex4An674jNlvv8/2bVp9YZ8TjtiXkU++DMDRh+zF38dMZfWaSt5570PeevcDvrFnZwCWr1wNQJOyUsrKSomIBrsOaxjLli3j5Zcnc9wJPwSgSdOmtGzZEiE+/XQ5AJ8uW0abNm3z2cyiIinnUojqJewl9QdGkBkFeykpAu6XNKA+PrOQ7NB+G7rt1pHJM+ZW1R207y4sXLKMt/6zGIAObbZm3vtLq7bPX7SU7dtuDUBJiZg0YgD/GTeEpyfNZvKMd6r2G3TmMbz0wEVcd/7xNG3iv7gVqvnz5lFevg2XX3IRJ51wLIMuv4QVK1bwvwMu5nc3XEePw7/LjTdcy6/P+03VMdOnTePE437AGb/8ORUVb+ax9YVJJcq5FKL66tn3Bb4REUMi4p6kDAH2T7ZVS1I/SVMkTan84LV6alp+bdWsKfff8HMuvOEhli1fVVV/Uq/u/P3JKTmdY9264MCTh7Brz0vpvueOdN2lPQCX/3EUex93Jd/+yfWUb70V55/+vXq5Bqt/a9dWMnvWTE48+RRGPvQozZo1Y9jtQxn5wP1c2P8inho3gQv7X8Sgyy4B4Gtd9+DJsU/z90dGccqPT+W8s8/M8xUUnrrq2UvqJGm8pJmSXpN0TlK/jaSxkt5MfpYn9UpGPyokTZe0b9a5+iT7vympT1b9fpJeTY65WTn8daO+wn4dsH019e2TbdWKiKER0T0iupe13qOempY/ZWUl3H/DL3jgiSk89vS/q+pLS0vofdjePDhmalXd/MUf03G78qr3HdqW896ij79wvo8/XcmEKW/Q41tdgcxQEcDqNZXc9dgkuu/RuR6vxupTu3bb0a7dduy1194AHNGjF7NnzeT/HnuEw4/oAUCPnkcy49XpADRv3pyvbLUVAAd/57tUVlaydOmS/DS+QNXhME4lcH5EdAUOBM6U1BUYAIyLiC7AuOQ9wJFAl6T0A25N2rMNMBA4gExHeeD6XxDJPr/IOq7XphpVX2F/LjBO0hOShiblSTIXeE49fWaj95eBP+b1t9/n5nue/kL9YQfsxhtzFzJ/0UdVdY8/M50Te+5L0yZl7Lj9tuy6Qxsmz5hL6/LmbN28GQBbbtGEww/YndfnLgRgu9Ytq47/waF7MfOt9+r/oqxetG7Thnbbbcfct+cA8OKkF9h5l11o07YtUya/BMBLL05ihx07A/DB4sVV925enT6ddevW0apVebXntupJuZeaRMSCiJiavF4GzAI6AL2B4cluw4Fjk9e9gbsiYxLQSlJ7oCcwNiKWRMRSYCzQK9nWMiImReY/+l1Z59qoehnUjYgnJX2VzG+jDkn1fGByRKytj89s7L7VbWd+fPQBvPrGfCaNyPxCH3jLKMY8O5MTe+5XdWN2vVlz3uehp17hlYcuoXLtOs4dMpJ164LtWrfktsGnUlpSQkmJeGjsVJ741wwA/nZ1H1qXt0CC6a/P4+yrRzT4dVrdGXDxZVzU/wLWrFlDx46dGHzVNRxy6OFcN+S3rK2spOkWW3D5oMEAjH1qDCMfuJ+y0lK22HJLrr3hpoK9kZgvtfn3JakfmV74ekMjYmg1+3UG9gFeBNpFxIJk0/tAu+R1B+DdrMPmJXU11c+rpr7mNjfWmRzN9jmrcTbM8mrp5Fvy3QRrhLYs+/Jfidqt/5icM+f1a3tu8vMkNQcmAFdHxMOSPoqIVlnbl0ZEuaR/AEMi4tmkfhzQHzgE2DIirkrqLwNWAs8k+38vqT8Y6B8RR9fUnuJeDMLMLEd1NYyTOZeaAA8B90bEw0n1wmQIhuTnoqR+PtAp6/COSV1N9R2rqa+Rw97MjMyU5lxLTZKZMXcAsyLipqxNo4D1M2r6AI9l1Z+WzMo5EPg4Ge4ZA/SQVJ7cmO0BjEm2fSLpwOSzTss610Z5IraZGbn12HN0EHAq8KqkaUndxcAQYKSkvsA7wEnJttHAUUAFsAI4HSAilki6Epic7Dc4ItZPsToDuBNoBjyRlBo57M3MqLvlEpKx942d7PBq9g+g2i9GRMQwYFg19VOAPWvTLoe9mRl12rNvlBz2Zmb44SVmZqngnr2ZWQoU+5fQHPZmZrhnb2aWCu7Zm5mlQJFnvcPezAzY5DdjC53D3swMD+OYmaVCkWe9w97MDNyzNzNLhSLPeoe9mRn4Bq2ZWSp4GMfMLAUc9mZmKVDkWe+wNzMD9+zNzFKhyLPeYW9mBp6NY2aWCiVF3rV32JuZ4WEcM7NU8A1aM7MUKPIhe4e9mRn4Bq2ZWSoIh72ZWdEr8o69w97MDHyD1swsFYo86x32ZmbgL1WZmaWCZ+OYmaVAkXfsHfZmZuBhHDOzVCjuqIeSmjZKWibpk2rKMkmfNFQjzczqm6ScSw7nGiZpkaQZWXWDJM2XNC0pR2Vtu0hShaTXJfXMqu+V1FVIGpBVv5OkF5P6ByQ13VSbagz7iGgRES2rKS0iouUmr9jMrECUKPeSgzuBXtXU/y4iuiVlNICkrsDJwB7JMX+WVCqpFPgTcCTQFTgl2Rfg2uRcuwJLgb6bvL6cmp2Q1FbSDutLbY41M2vMSkqUc9mUiJgILMnxo3sDIyLis4h4G6gA9k9KRUTMiYjVwAigtzJ/tTgMeDA5fjhw7CavL5eWSPqBpDeBt4EJwFzgiRwvxMys0avNMI6kfpKmZJV+OX7MWZKmJ8M85UldB+DdrH3mJXUbq98W+CgiKjeor1GuPfsrgQOBNyJiJ+BwYFKOx5qZNXq1GcaJiKER0T2rDM3hI24FdgG6AQuAG+vzejaUa9iviYgPgRJJJRExHuhej+0yM2tQdXmDtjoRsTAi1kbEOuA2MsM0APOBTlm7dkzqNlb/IdBKUtkG9TXKNew/ktQcmAjcK+kPwPIcjzUza/RUi7JZ55faZ709Dlg/U2cUcLKkLSTtBHQBXgImA12SmTdNydzEHRURAYwHfpgc3wd4bFOfn+s8+97AKuA84MfA1sDgHI81M2v0SutwuQRJ9wOHAK0lzQMGAodI6gYEmfuevwSIiNckjQRmApXAmRGxNjnPWcAYoBQYFhGvJR/RHxgh6SrgFeCOTbUpp7CPiOxe/PBcjjEzKyR1ucRxRJxSTfVGAzkirgaurqZ+NDC6mvo5fD4MlJOcwl7SMjK/jQCaAk2A5Z5rb2bFoshXS8i5Z99i/etkjmdvMrNzzMyKQrGvjVOrL1UBRMajQM9N7WtmViik3EshynUY5/istyVkpl2uqpcWJZZOvqU+T28F6tNVlZveyVJny+Zffk1HP5Yw45is15Vk7iT3rvPWmJnlSanDHoDbI+K57ApJBwGL6r5JZmYNr8gfVJXzmP0fc6wzMytIdbzqZaNTY89e0jeBbwFtJP0ma1NLMpP8zcyKQtrH7JsCzZP9WmTVf8LnX9U1Myt4hdpjz1WNYR8RE4AJku6MiHcaqE1mZg2uyDv2OY/Z3y6p1fo3ksoljamfJpmZNbwyKedSiHKdjdM6Ij5a/yYilkpqWz9NMjNreAWa4TnLNezXSdohIv4DIKkzn6+VY2ZW8Ip9uYRcw/4S4FlJE8gs53wwkOtjuMzMGr0iz/qcF0J7UlJ3MgH/CvAosLIe22Vm1qBSPRtnPUk/B84h8/iraWRWvHyBzBPOzcwKXl0+vKQxynU2zjnAN4B3IuJQYB/go/pqlJlZQ0v1N2izrIqIVcnDdreIiNmSdqvXlpmZNSBt9tNlC0OuYT8vmWf/KDBW0lLAX7Iys6JRqD32XOV6g/a45OUgSePJPHD8yXprlZlZA3PYbyBZQsHMrKikfSE0M7NUKK31Q1oLi8PezAx/g9bMLBU8Zm9mlgJF3rF32JuZAZR4nr2ZWfFzz97MLAXKinzQ3mFvZoZ79mZmqeCpl2ZmKVDkWe+wNzOD3Nd7L1TFfn1mZjkpkXIumyJpmKRFkmZk1W0jaaykN5Of5Um9JN0sqULSdEn7Zh3TJ9n/TUl9sur3k/RqcszNymFhH4e9mRl1G/bAnUCvDeoGAOMiogswLnkPcCTQJSn9gFsh88sBGAgcAOwPDFz/CyLZ5xdZx234Wf99fbm02sys2KkWZVMiYiKwZIPq3sDw5PVw4Nis+rsiYxLQSlJ7oCcwNiKWRMRSYCzQK9nWMiImRUQAd2Wda6Mc9mZmZG7Q5l7UT9KUrNIvh49oFxELktfvA+2S1x2Ad7P2m5fU1VQ/r5r6GvkGrZkZtVvPPiKGAkM397MiIiTF5h6/OdyzNzMjE4a5ls20MBmCIfm5KKmfD3TK2q9jUldTfcdq6mvksDczo85v0FZnFLB+Rk0f4LGs+tOSWTkHAh8nwz1jgB6SypMbsz2AMcm2TyQdmMzCOS3rXBvlYRwzM+r2sYSS7gcOAVpLmkdmVs0QYKSkvsA7wEnJ7qOBo4AKYAVwOkBELJF0JTA52W9wRKy/6XsGmRk/zYAnklJzmzI3cxufVZU0zoZZXn26qjLfTbBGqHXzsi+d1A//e0HOmXP83u0L7vu27tmbmeEHjpuZpUJxR73D3swMgFL37M3Mil+RZ73D3swMQEU+kOOwNzPDPXszs1Qocc/ezKz4uWdvZpYCfgatmVkKlBR31jvszczAs3HMzFKhyEdxHPb5NvftOfzv+edVvZ83713OOOvXfPTRRzwzfhwlKqF822258upraNu2HW/PeYvLL72YWTNf4+xzzqPP6X3z2HqrSwvfX8CVl1/E0iUfgkTv407kpB+dyi2/v4HnJj5DkyZN6NCxExcPuooWLVoyc8Z0rr16UObgCH7W70y+e9j3qs63du1a+p56Em3atOP6P/w5PxdVQIq9Z+9VLxuRtWvXcsSh3+GeESNp2XJrmjdvDsC999zFnLcquGzgYD788EMWvDef8U+Po2XLlqkL+2Je9fKDxYv58IPF7Pa1rixfvpy+PzmRa268mUULF7LfNw6grKyMP998IwBn/Pp8Vq1cSVmTJpSVlfHB4sX0OeV4HntyPGVlmT7ciHvuZPbM11i+fHnRh31drHo58Y0lOWfOd766TcH9ZvDDSxqRFye9QKdOndh++w5VQQ+wauXKqhX5tt12W/b8+l5V/0Nb8Wjdpg27fa0rAFtttRU77rQzixct4oBvHlT133uPPfdm0cKFAGzZrFlV/erVn31h1cZFC9/n+WcncsyxJzTwVRSuBnh4SV45MRqRJ594nF5HHV31/o9/+B3/N+pRmjdvwe1/uyuPLbOGtuC9+bw5exZ77LnXF+ofH/Uwh/c4sur9a69O57eDL2Xhgve4bPCQqvD/w41DOOOc81mxfHmDtruQFWaE567Be/aSTq9hW9UT2++4bbOf5VuQ1qxezYTxT9OjZ6+qurPPOY+nxk3g+0cfw4j77slj66whrVixnEsuPJdfXzCArbL+hjf8jr9SWlpGjyM/7xDs8fW9uPfvo7j97ge4+87b+Oyzz3hu4jOUl2/D7l/bIx/NL1jF3rPPxzDOFRvbEBFDI6J7RHTv+4t+DdmmvHv22Yns3nUPtm3d+r+2HfX9Y/jn2Kfy0CpraJVr1nDJhefS48jvc8hhR1TVPz7qEZ771wQGXnVttQ/Z6LzTLjRr9hXmvPUm0//9Cs9OfIYTjj6CgRdfwMuTX+SKS/s35GUUJNWiFKJ6GcaRNH1jm4B29fGZhe6J0Y9z5FHfr3r/zjtz2XHHzgCMHz+OnXbaOU8ts4YSEVxz5eXsuNPOnPyTn1bVT3r+X9x31zBuuW04WzZrVlX/3vx5tG23HWVlZby/4D3emfs27dt34Fdnn8evzs7M8Jo65SXuv/tOBl51bUNfTuEp1BTPUX2N2bcDegJLN6gX8Hw9fWbBWrFiBZOef57LBg6uqvvDTTcyd+7blJSI9u07cOnAzF+IPli8mFP+3wks//RTSkpKuOfu4TwyavQXbuhaYZo+bSpPPj6KXXb9Kn1OOR6AX555Lr+//resWbOGc8/4OQB7fH1v/vfigUyfNpW777ydsrIySlTCBQMuo1V5eT4voaAV6vBMrupl6qWkO4C/RcSz1Wy7LyJ+tKlzpHHqpW1aMU+9tM1XF1MvJ8/5OOfM+cbOWxfcb4Z66dlHxEYnf+cS9GZmDa7g4rt2PPXSzIzi/watw97MDK+NY2aWCkWe9Q57MzOg2u8vFBOHvZkZHsYxM0uFIs96h72ZGVD0ae+wNzPDUy/NzFLBY/ZmZilQ7GHvJ1WZmZEZxsn1n02eS5or6VVJ0yRNSeq2kTRW0pvJz/KkXpJullQhabqkfbPO0yfZ/01Jfb7M9TnszczI9OxzLTk6NCK6RUT35P0AYFxEdAHGJe8BjgS6JKUfcGumPdoGGAgcAOwPDFz/C2JzOOzNzGiQh5f0BoYnr4cDx2bV3xUZk4BWktqTWSZ+bEQsiYilwFigF5vJYW9mBrVK++xHqCZlw0frBfCUpJeztrWLiAXJ6/f5/EFOHYB3s46dl9RtrH6z+AatmRm1e3hJRAwFanpQ9rcjYr6ktsBYSbM3OD4kNegzO9yzNzOjbodxImJ+8nMR8AiZMfeFyfAMyc9Fye7zgU5Zh3dM6jZWv1kc9mZmUGdpL2krSS3WvwZ6ADOAUcD6GTV9gMeS16OA05JZOQcCHyfDPWOAHpLKkxuzPZK6zeJhHDMz6vQbtO2AR5JVNMuA+yLiSUmTgZGS+gLvACcl+48GjgIqgBXA6QARsUTSlcDkZL/BEbFkcxtVL8+grQt+Bq1Vx8+gterUxTNoKxatzDlzdm3brOC+guWevZkZRb8OmsPezAz88BIzs1Qo8qx32JuZgYdxzMzSocjT3mFvZoYfXmJmlgoeszczS4ESh72ZWRoUd9o77M3M8DCOmVkqFHnWO+zNzMA9ezOzVPByCWZmKVDcUe+wNzMDPIxjZpYK/gatmVkaFHfWO+zNzKDos95hb2YGUFLkg/YOezMziv8GbUm+G2BmZvXPPXszM4q/Z++wNzPDUy/NzFLBPXszsxRw2JuZpYCHcczMUsA9ezOzFCjyrHfYm5kBRZ/2DnszM4p/uQRFRL7bYJsgqV9EDM13O6xx8Z8Lqw0vl1AY+uW7AdYo+c+F5cxhb2aWAg57M7MUcNgXBo/LWnX858Jy5hu0ZmYp4J69mVkKOOzNzFLAYd/ISeol6XVJFZIG5Ls9ln+ShklaJGlGvttihcNh34hJKgX+BBwJdAVOkdQ1v62yRuBOoFe+G2GFxWHfuO0PVETEnIhYDYwAeue5TZZnETERWJLvdlhhcdg3bh2Ad7Pez0vqzMxqxWFvZpYCDvvGbT7QKet9x6TOzKxWHPaN22Sgi6SdJDUFTgZG5blNZlaAHPaNWERUAmcBY4BZwMiIeC2/rbJ8k3Q/8AKwm6R5kvrmu03W+Hm5BDOzFHDP3swsBRz2ZmYp4LA3M0sBh72ZWQo47M3MUsBhb42apEMk/SN5/YOaVv6U1ErSGZvxGYMkXfBl2mnW2DnsLS+SFT1rJSJGRcSQGnZpBdQ67M3SwGFvdU5SZ0mzJd0raZakByV9RdJcSddKmgqcKKmHpBckTZX0d0nNk+N7JcdPBY7POu9PJd2SvG4n6RFJ/07Kt4AhwC6Spkm6PtnvQkmTJU2XdEXWuS6R9IakZ4HdGvBfj1lelOW7AVa0dgP6RsRzkobxeY/7w4jYV1Jr4GHgexGxXFJ/4DeSrgNuAw4DKoAHNnL+m4EJEXFc8reE5sAAYM+I6AYgqQfQhcxS0QJGSfoOsJzM0hPdyPw/MBV4uU6v3qyRcdhbfXk3Ip5LXt8D/Dp5vT68DyTzQJbnJAE0JbMEwO7A2xHxJoCke4B+1Zz/MOA0gIhYC3wsqXyDfXok5ZXkfXMy4d8CeCQiViSf4fWGrOg57K2+bLgOx/r3y5OfAsZGxCnZO0nqVodtEHBNRPx1g884tw4/w6wgeMze6ssOkr6ZvP4R8OwG2ycBB0naFUDSVpK+CswGOkvaJdnvFKo3DvhVcmyppK2BZWR67euNAX6WdS+gg6S2wETgWEnNJLUAjvkyF2pWCBz2Vl9eB86UNAsoB27N3hgRi4GfAvdLmk4yhBMRq8gM2zye3KBdtJHznwMcKulVMuPtXSPiQzLDQjMkXR8RTwH3AS8k+z0ItIiIqWSGk/4NPEFmKWmzouZVL63OSeoM/CMi9sx3W8wswz17M7MUcM/ezCwF3LM3M0sBh72ZWQo47M3MUsBhb2aWAg57M7MU+P/dICgl9aILOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(solutions['actual'], solutions['predicted'], rownames=['actual'], colnames=['predicted'])\n",
    "%matplotlib inline\n",
    "sns.heatmap(confusion_matrix, annot=True,cmap='Blues', fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy  0.9538536744337625\n"
     ]
    }
   ],
   "source": [
    "print( \"Test Accuracy \", (confusion_matrix.loc[0,0] + confusion_matrix.loc[1,1]) / (confusion_matrix.loc[0,0] + confusion_matrix.loc[1,1]+ confusion_matrix.loc[1,0] + confusion_matrix.loc[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Review\n",
    "Lets take a look at some of those word encoding vectors and see if they learned some sort of pattern.  Famously GloVe embeddings have similar distances between king and queen as they do between him and her.  In this case the focus of the training wasn't nessisarily the words but the end results, in either way we can evaluate distances of synonmys and antonmys to see if we have some pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.get_variable_names()\n",
    "embedding_lookup = classifier.get_variable_value('EmbedSequence/embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "synonmys_w = ['perhaps', 'maybe', 'possibly', 'conceivably', 'feasibly', 'possible']\n",
    "antonymns_w = ['certainly','inevitably', 'certain', 'inevitable','definitely','surely' ]\n",
    "random_v = [ random.randint(1,len(rev_token)) for _ in range(6)]\n",
    "\n",
    "synonmys_v = [token.word_index[w] for w in  synonmys_w]\n",
    "antonymns_v = [token.word_index[w] for w in  antonymns_w]\n",
    "random_w = [rev_token[w] for w in  random_v]\n",
    "\n",
    "synonmys_e = [embedding_lookup[i] for i in synonmys_v]\n",
    "antonymns_e = [embedding_lookup[i] for i in antonymns_v]\n",
    "random_e = [embedding_lookup[i] for i in random_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance Function\n",
    "Here we will calculate the distance between respective words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29033905, 0.228878, 0.47191006, 0.16608858, 0.39882907, 0.34883836, 0.23140863, 0.603239, 0.32845628, 0.5290239, 0.3061973, 0.4427857, 0.5910614, 0.29337338, 0.5072404, 0.20371372, 0.30845058, 0.2107647, 0.18652174, 0.4145062, 0.30739146, 0.42299324, 0.16694796, 0.5783617, 0.51848376, 0.2668382, 0.38881835, 0.2787532, 0.66194874, 0.2371056]\n",
      "[0.1910904, 0.29033905, 0.228878, 0.47191006, 0.16608858, 0.39882907, 0.34883836, 0.45970267, 0.23140863, 0.603239, 0.32845628, 0.5290239, 0.3061973, 0.4427857, 0.21832639, 0.5910614, 0.29337338, 0.5072404, 0.20371372, 0.30845058, 0.2107647, 0.48156407, 0.18652174, 0.4145062, 0.30739146, 0.42299324, 0.16694796, 0.5783617, 0.27596685, 0.51848376, 0.2668382, 0.38881835, 0.2787532, 0.66194874, 0.2371056, 0.3856838]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def distance(v1, v2) :\n",
    "    return np.linalg.norm(v1-v2)\n",
    "        \n",
    "synonmys_dist = distance(synonmys_v[0], synonmys_v[1])\n",
    "\n",
    "result = []\n",
    "for i in range(len(synonmys_e)):\n",
    "    for j in range(len(antonymns_e)):\n",
    "        if i != j:\n",
    "            result.append(distance(synonmys_e[i], antonymns_e[j]))\n",
    "\n",
    "print(result)\n",
    "print(([distance(v1,v2) for v1 in synonmys_e for v2 in antonymns_e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Distance\n",
    "Lets try to look at all the words and find the 10 closest and 10 farthest from each other.  Obviously this is going to be computationally complex considering the size of the matrix and this being an n^2 problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest = [(100,1,1)] * 10\n",
    "largest = [(-100,1,1)] * 10\n",
    "\n",
    "words_count = len(embedding_lookup)\n",
    "\n",
    "for i in range(118577,words_count):\n",
    "    for j in range(i+1, words_count):\n",
    "\n",
    "        dist_ = distance(embedding_lookup[i], embedding_lookup[j]) \n",
    "        if dist_ < smallest[9][0]:\n",
    "            smallest[9] = (dist_,i,j)\n",
    "            smallest = sorted(smallest)\n",
    "        if dist_ > largest[0][0]:\n",
    "            largest[0] = (dist_,i,j)\n",
    "            largest = sorted(largest)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARGEST\n",
      "'fucking' to 'stranger' distance: 2.303\n",
      "'conform' to 'cocksucker' distance: 2.312\n",
      "'reconsider' to 'faggots' distance: 2.314\n",
      "'fucking' to 'saint' distance: 2.321\n",
      "'reconsider' to 'cocksucker' distance: 2.323\n",
      "'fucking' to 'recommend' distance: 2.335\n",
      "'conform' to 'fucked' distance: 2.344\n",
      "'reconsider' to 'fucked' distance: 2.360\n",
      "'fucking' to 'conform' distance: 2.404\n",
      "'fucking' to 'reconsider' distance: 2.420\n",
      "\n",
      "SMALLEST\n",
      "'woolsack' to 'neckbearded' distance: 0.019\n",
      "'ruffner' to 'signallling' distance: 0.020\n",
      "'rafique' to 'siddiq' distance: 0.020\n",
      "'destress' to 'terminalogy' distance: 0.020\n",
      "'eunapiuss' to 'youngstars' distance: 0.020\n",
      "'elollipop' to 'colectives' distance: 0.020\n",
      "'rosenfield' to 'erasmian' distance: 0.020\n",
      "'flamengo' to 'limehouse' distance: 0.021\n",
      "'semai' to 'vase' distance: 0.021\n",
      "'pervy' to 'persnickety' distance: 0.021\n"
     ]
    }
   ],
   "source": [
    "print(\"LARGEST\")\n",
    "for d,v1,v2 in largest:\n",
    "    print( f\"\\'{rev_token[v1]:}\\' to \\'{rev_token[v2]:}\\' distance: {d:.3f}\")\n",
    "print(\"\\nSMALLEST\")\n",
    "for d,v1,v2 in smallest:\n",
    "    print( f\"\\'{rev_token[v1]:}\\' to \\'{rev_token[v2]:}\\' distance: {d:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Distance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2868683\n",
      "0.25882998\n"
     ]
    }
   ],
   "source": [
    "king_v = embedding_lookup[token.word_index['king']]\n",
    "queen_v = embedding_lookup[token.word_index['queen']]\n",
    "\n",
    "his_v = embedding_lookup[token.word_index['man']]\n",
    "her_v = embedding_lookup[token.word_index['woman']]\n",
    "\n",
    "print(distance(king_v,queen_v))\n",
    "print(distance(his_v,her_v))\n",
    "\n",
    "\n",
    "# distance(embedding_lookup[token.word_index['fucking']],embedding_lookup[token.word_index['fucked']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # var = [v for v in tf.trainable_variables() if v.name == \"tower_2/filter:0\"][0]\n",
    "# var = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "# print(var)\n",
    "# [n.name for n in tf.get_default_graph().as_graph_def().node]\n",
    "# # embedded_words = sess.run(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network\n",
    "This contains some steps to build and train the network by hand, this method has some benifits, such as the ability to control a dynamic learning rate, or optimize the stop criteria, but runs much to slow from jupyter notebooks and is a known problem with tensorflow were each re-trainging bactch reloads the graph, causing significant time delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_learning_rate = 0.1\n",
    "# global_step = tf.Variable(0, trainable=False)\n",
    "# increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "# lr_schedule = tf.train.exponential_decay(initial_learning_rate, global_step, 100, 0.95, staircase=False)\n",
    "# lr = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EPOC = 1\n",
    "# TRAINING_SIZE = encode_text.shape[0]\n",
    "# BATCH_SIZE = 1024\n",
    "\n",
    "# x = tf.placeholder(tf.int32,[BATCH_SIZE,encode_text.shape[1]])\n",
    "# y = tf.placeholder(tf.int32,[BATCH_SIZE])\n",
    "# w_vect = embed(x)\n",
    "# outputs = encode(w_vect)\n",
    "# encoding, alphas = attend(outputs, 128,2)\n",
    "# logits = tf.layers.dense(encoding, 2 , activation=None)\n",
    "# predicted = tf.argmax(logits, 1)\n",
    "\n",
    "# y_ = tf.one_hot(y, 2,1,0)\n",
    "\n",
    "# # accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "# correct_prediction = tf.equal(predicted, tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# loss = tf.losses.softmax_cross_entropy(onehot_labels=y_, logits=logits)\n",
    "# # train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "# train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "# init_op = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# errors = []\n",
    "# accs = []\n",
    "# errors_test = []\n",
    "# accs_test = []\n",
    "# learning_rate = 0.1\n",
    "# decay_rate = 0.99\n",
    "# min_rate = 0.0001\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op)\n",
    "#     for i in range(EPOC):\n",
    "#         start = 0\n",
    "#         print(f\"Running EPOC {i}\")\n",
    "# #         print('Learning rate: %f' % (sess.run(trainer ._lr)))\n",
    "#         for end in range(BATCH_SIZE,TRAINING_SIZE,BATCH_SIZE):\n",
    "#             x_train_sub = train_x[start:end]\n",
    "#             y_train_sub = train_y[start:end]\n",
    "#             _, error_value, accs_value = sess.run([train_op, loss, accuracy], \n",
    "#                                                   feed_dict={x: x_train_sub, \n",
    "#                                                              y: y_train_sub,\n",
    "#                                                              lr:learning_rate})\n",
    "#             test_error_value, test_accs_value = sess.run([loss, accuracy], \n",
    "#                                                   feed_dict={x: test_x[:BATCH_SIZE], \n",
    "#                                                              y: test_y[:BATCH_SIZE]})\n",
    "                \n",
    "#             accs.append(accs_value)\n",
    "#             errors.append(error_value)\n",
    "#             errors_test.append(test_error_value)\n",
    "#             accs_test.append(accs_test)\n",
    "#             print(f\"Loss: {error_value:.3f} \\t Acc:{accs_value:.3f}\\t Test Loss: {test_error_value:.3f} \\t Test Acc:{test_accs_value:.3f}\\t lr:{learning_rate:.5f}\")\n",
    "#             start = end\n",
    "            \n",
    "#             learning_rate = learning_rate * decay_rate\n",
    "#             if learning_rate < min_rate:\n",
    "#                 learning_rate = min_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# sns.lineplot(x=list(range(0,len(errors))), y=errors)\n",
    "# sns.lineplot(x=list(range(0,len(accs))), y=accs)\n",
    "# sns.lineplot(x=list(range(0,len(errors_test))), y=errors_test)\n",
    "# sns.lineplot(x=list(range(0,len(accs_test))), y=accs_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
